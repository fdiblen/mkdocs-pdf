{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#guide","title":"Guide","text":"<p>This is a guide to software development at the Netherlands eScience Center. It both serves as a source of information for how we work at the eScience Center, and as a basis for discussions and reaching consensus on this topic.</p> <p>Read The Turing Way instead</p> <p>If you are looking for an overall picture of best practices, read The Turing Way first. We joined forces with that guide for most of our generic research software engineering advice. Because The Turing Way is programming language agnostic, this guide provides addtional specific language guides. We also provide technology guides on digital technologies we use often in our projects with research partners. Please be aware that most remaining best practices content is unmaintained, be careful when using it. We plan on removing that content (#286).</p> <p>If you would like to contribute to this book see CONTRIBUTING.md.</p>"},{"location":"best_practices/code_quality/","title":"Code Quality","text":"<p>Ways to improve code quality are in the Code quality chapter on the Turing Way.</p> <p>There are online software quality improvement tools see the language guides for good options per language.</p>"},{"location":"best_practices/code_quality/#editorconfig","title":"Editorconfig","text":"<p>The eScience Center has a shared editor config file</p>"},{"location":"best_practices/code_quality/#name-spaces","title":"Name spaces","text":"<p>If your language supports namespaces, use nl.esciencecenter or better a namespace based on the project.</p>"},{"location":"best_practices/code_quality/#code-reviews","title":"Code reviews","text":"<p>See the Code Reviews section.</p>"},{"location":"best_practices/code_review/","title":"Code reviews","text":""},{"location":"best_practices/code_review/#introduction","title":"Introduction","text":"<p>At the eScience Center, we value software quality. Higher quality software has fewer defects, better security, and better performance, which leads to happier users who can work more effectively.</p> <p>Code reviews are an effective method for improving software quality. McConnell (2004) suggests that unit testing finds approximately 25% of defects, function testing 35%, integration testing 45%, and code review 55-60%. While that means that none of these methods are good enough on their own, and that they should be combined, clearly code review is an essential tool here.</p> <p>Code review also improves the development process. By reviewing new additions for quality, less technical debt is accumulated, which helps long-term maintainability of the code. Reviewing lets developers learn from each other, and spreads knowledge of the code around the team. It is also a good means of getting new developers up to speed.</p> <p>The main downside of code reviews is that they take time and effort. In particular, if someone from outside the project does the reviewing, they'll have to learn the code, which is a significant investment. Once up to speed, the burden is reduced significantly however, and the returns include a much smaller amount of time spent debugging later.</p>"},{"location":"best_practices/code_review/#approach","title":"Approach","text":"<p>It's important to distinguish between semi-formal code reviews and formal code inspections. The latter involve \"up to six participants and hours of meetings paging through detailed code printouts\" (SMARTBEAR 2016). As this extra formality does not seem to yield better results, we limit ourselves to light-weight, informal code reviews.</p>"},{"location":"best_practices/code_review/#process","title":"Process","text":"<p>We haven't yet decided on how to integrate code reviews into our working process. While that gets hashed out, here is some general advice from various sources and experience.</p> <ul> <li> <p>Review everything, nothing is too short or simple</p> </li> <li> <p>Try to have something else to do, and spread the load throughout your working day. Don't review full-time.</p> </li> <li> <p>Don't review for more than an hour at a time, after that the success rate drops quite quickly</p> </li> <li> <p>Don't review more than 400 lines of code (LOC) at a time, less than 200 LOC is better</p> </li> <li> <p>Take the time, read carefully, don't review more than 500 LOC / hour</p> </li> </ul>"},{"location":"best_practices/code_review/#prerequisites","title":"Prerequisites","text":"<p>Before handing over a change or a set of code for review, the following items should be there for the reviewer to work with:</p> <ul> <li>Documentation on what was changed and why (feature, bug, issue #, etc.)</li> <li>Comments / annotations by the author on the code itself</li> <li>Test cases</li> </ul> <p>Also, before doing a code review, make sure any tools have run that check the code automatically, e.g. checkers for coding conventions and static analysis tools, and the test suite. Ideally, these are run as part of the continuous integration infrastructure.</p>"},{"location":"best_practices/code_review/#review-checklist","title":"Review checklist","text":"<p>This section provides two checklists for code reviews, one for the whole program, and one for individual files or proposed changes.</p> <p>In all cases, the goal is to use your brain and your programming experience to figure out how to make the code better. The lists are intended to be a source of inspiration and a description of what should be best practices in most circumstances. Some items on this list may not apply to your project or programming language, in which case they should be disregarded.</p>"},{"location":"best_practices/code_review/#excluded-from-this-checklist","title":"Excluded from this checklist","text":"<p>The following items are part of a software quality check, but are better done by an automated tool than by a human. As such, they've been excluded from this checklist. If tools are not available, they should be checked manually.</p> <ul> <li>Coding conventions (e.g. PEP 8)</li> <li>Test coverage</li> </ul>"},{"location":"best_practices/code_review/#rubric-for-assessing-code-quality","title":"Rubric for assessing code quality","text":"<p>All code should be level 3 or 4.</p> Level 1 2 3 4 names names appear unreadable, meaningless or misleading names accurately describe the intent of the code, but can be incomplete, lengthy, misspelled or inconsistent use of casing names accurately describe the intent of the code, and are complete, distinctive, concise, correctly spelled and consistent use of casing all names in the program use a    consistent vocabulary headers headers are generally missing or descriptions are redundant or obsolete; use mixed languages or are misspelled header comments are    generally present; summarize the goal of parts of the program and how to use those; but may be somewhat inaccurate or incomplete header comments are generally present; accurately summarize the role of parts of the program and how to use those; but may still be wordy header comments are generally present; contain only essential explanations, information and references comments comments are generally missing, redundant or obsolete; use mixed languages or are misspelled comments explain code and potential problems, but may be wordy comments explain code and potential problems, are concise comments are only present where strictly needed layout old commented out code is present or lines are generally too long to read positioning of elements within source files is not optimized for readability positioning of elements within source files is optimized for readability positioning of elements is consistent between files and in line with platform conventions formatting formatting is missing or misleading indentation,   line    breaks, spacing and brackets highlight the intended structure but erratically indentation, line breaks, spacing and brackets consistently highlight the intended structure formatting makes similar parts of code clearly identifiable flow there is deep nesting; code performs more than one task per line; unreachable code is present flow is complex or contains many exceptions or jumps; parts of code are duplicate flow is simple and contains few exceptions or jumps; duplication is very limited in the case of exceptions or jumps, the most common path through the code is clearly visible idiom control structures are customized in a misleading way choice of control structures is inappropriate choice of control structures is appropriate; reuse of library functionality may be limited reuse of library functionality and generic data structures where possible expressions expressions are repeated or contain unnamed constants expressions are complex or long; data types are inappropriate expressions are simple; data types are appropriate expressions are all essential for control flow decomposition most code is in one or a few big routines; variables are reused for different purposes most routines are limited in length but mix tasks; routines share many variables instead of having parameters routines perform a limited set of tasks divided into parts; use of shared variables is limited routines perform a very limited set of tasks and the number of parameters and shared variables is limited modularization most code is in one or a few large modules; or modules are    artificially separated modules have mixed responsibilities, contain many variables or contain many routines modules have clearly defined responsibilities, contain few variables and a somewhat limited amount of routines modules are defined such that communication between them is limited <ul> <li>no need to assess a level that is not relevant to the software</li> <li>level 2 implies that the features in level 1 are    not present, level 4 implies that the   features in level 3 are also present</li> </ul> <p></p> <p>This rubric is based on:</p> <p>Stegeman, Barendsen, &amp; Smetsers (2016). Designing a rubric for feedback on code quality in programming courses. In proceedings of the 16th Koli Calling International Conference on Computing Education Research. ACM.</p>"},{"location":"best_practices/code_review/#program-level-checklist","title":"Program level checklist","text":"<p>Here is a list of things to consider when looking at the program as a whole, rather than when looking at an individual file or change.</p>"},{"location":"best_practices/code_review/#documentation","title":"Documentation","text":"<p>Documentation is a prerequisite for using, developing and reviewing the program. Here are some things to check for.</p> <ul> <li>Is there a description of the purpose of the program or library?</li> <li>Are detailed requirements listed?</li> <li>Are requirements ranked according to MoSCoW?</li> <li>Is the use and function of third-party libraries documented?</li> <li>Is the structure/architecture of the program documented? (see below)</li> <li>Is there an installation manual?</li> <li>Is there a user manual?</li> <li>Is there documentation on how to contribute?</li> <li>Including how to submit changes</li> <li>Including how to document your changes</li> </ul>"},{"location":"best_practices/code_review/#architecture","title":"Architecture","text":"<p>These items are mainly important for larger programs, but may still be good to consider for small ones as well.</p> <ul> <li>Is the program split up into clearly separated modules?</li> <li>Are these modules as small as they can be?</li> <li>Is there a clear, hierarchical or layered, dependency structure between   these modules?</li> <li>If not, functionality should be rearranged, or perhaps heavily     interdependent modules should be combined</li> <li>Can the design be simplified?</li> </ul>"},{"location":"best_practices/code_review/#security","title":"Security","text":"<p>If you're making software that is accessible to the outside world (e.g. a web application), then security becomes important. Security issues are defects, but not all defects are security issues. A security-conscious design can help mitigate the security impact of defects.</p> <ul> <li>Which modules deal with user input?</li> <li>Which modules generate output?</li> <li>Are input and output compartmentalised?</li> <li>If not, consider making separate modules that manage all input     and output, so validation can happen in one place</li> <li>In which modules is untrusted data present?</li> <li>The fewer the better</li> <li>Is untrusted data compartmentalised?</li> <li>Ideally, validate in the input module and pass only     validated data to other parts</li> </ul>"},{"location":"best_practices/code_review/#legal","title":"Legal","text":"<p>\"I'm an engineer, not a lawyer!\" is an oft-overheard phrase, but being an engineer doesn't give you permission to ignore the legal rights of the creators of the code you're using. Here are some things to check. When in doubt, ask your licensing person for advice.</p> <ul> <li>Are the licenses of all modules/libraries that are used documented?</li> <li>Are the requirements set by those licenses fulfilled?</li> <li>Are the licenses included where needed?</li> <li>Are copyright statements included in the code where needed?</li> <li>Are copyright statements included in the documentation where needed?</li> <li>Are the licenses of all the parts compatible with each other?</li> <li>Is the project license compatible with all libraries?</li> </ul>"},{"location":"best_practices/code_review/#filechange-level-checklist","title":"File/Change level checklist","text":"<p>When you're checking individual changes (e.g. pull requests) or files, the code itself becomes the subject of scrutiny. Depending on the language, files may contain interfaces, classes or other type definitions, and functions. All these should be checked, as well as the file overall:</p> <ul> <li>Does this file contain a logical grouping of functionality?</li> <li>How big is it? Should it be split up?</li> <li>Is it easy to understand?</li> <li>Can any of the code be replaced by library functions?</li> </ul>"},{"location":"best_practices/code_review/#interfaces","title":"Interfaces","text":"<ul> <li>Is the interface documented?</li> <li>Does the concept it models make sense?</li> <li>Can it be split up further? (Interfaces should be as small as possible)</li> </ul> <p>Note that most of the following items assume an object-oriented programming style, which may not be relevant to the code you're looking at.</p>"},{"location":"best_practices/code_review/#classes-and-types","title":"Classes and types","text":"<ul> <li>Is the class documented?</li> <li>Does it have a single responsibility? Can it be split?</li> <li>If it's designed to be extended, can it be?</li> <li>If it's not designed to be extended, is it protected against that? (e.g. final declarations)</li> <li>If it's derived from another class, can you substitute an object of this class for one of its parent class(es)?</li> <li>Is the class testable?<ul> <li>Are the dependencies clear and explicit?</li> <li>Does it have a small number of dependencies?</li> <li>Does it depend on interfaces, rather than on classes?</li> </ul> </li> </ul>"},{"location":"best_practices/code_review/#functionmethod-declarations","title":"Function/Method declarations","text":"<ul> <li>Are there comments that describe the intent of the function or method?</li> <li>Are input and output documented? Including units?</li> <li>Are pre- and postconditions documented?</li> <li>Are edge cases and unusual things commented?</li> </ul>"},{"location":"best_practices/code_review/#functionmethod-definitions","title":"Function/Method definitions","text":"<ul> <li>Are edge cases and unusual things commented?</li> <li>Is there incomplete code?</li> <li>Could this function be split up (is it not too long)?</li> <li>Does it work? Perform intended function, logic correct, ...</li> <li>Is it easy to understand?</li> <li>Is there redundant or duplicate code? (DRY)</li> <li>Do loops have a set length and do they terminate correctly?</li> <li>Can debugging or logging code be removed?</li> <li>Can any of the code be replaced by library functions?</li> </ul>"},{"location":"best_practices/code_review/#security_1","title":"Security","text":"<ul> <li>If you're using a library, do you check errors it returns?</li> <li>Are all data inputs checked?</li> <li>Are output values checked and encoded properly?</li> <li>Are invalid parameters handled correctly?</li> </ul>"},{"location":"best_practices/code_review/#tests","title":"Tests","text":"<ul> <li>Do unit tests actually test what they are supposed to?</li> <li>Is bounds checking being done?</li> <li>Is a test framework and/or library used?</li> </ul>"},{"location":"best_practices/code_review/#providing-feedback","title":"Providing feedback","text":"<p>The main purpose of a code review is to find issues or defects in a piece of code. These issues then need to be communicated back to the developer who proposed the change, so that they can be fixed. Doing this badly can quickly spoil everyone's fun.</p> <p>Perhaps the most important point in this guide therefore is that the goal of a code review is not to provide criticism of a piece of code, or even worse, the person who wrote it. The goal is to help create an improved version.</p> <p>So, when providing feedback, stay positive and constructive. Suggest a better way if possible, rather than just commenting that the current solution is bad. Ideally, submit a patch rather than an issue ticket. And always keep in mind that you're not required to find anything, if the code is fine, it's fine. If it's more than fine, file a compliment!</p> <p>Most of our projects are hosted on GitHub, so most results will be communicated through pull requests and issues there. However, if you find something particularly bad or weird, consider talking in person, where a lengthy, complicated, or politically sensitive explanation is easier to do.</p>"},{"location":"best_practices/code_review/#communicating-results-through-github","title":"Communicating results through GitHub","text":"<p>If you are reviewing a pull request on Github, comments should be added in the <code>Files changed</code> section, so they can be attached to a particular line of code. Make many small comments this way, rather than a big ball of text with everything in it, so that different issues can be kept separate. Where relevant, refer to existing Issues and documentation.</p> <p>If you're reviewing existing code rather than changes, it is still handy to use pull requests. If you find an issue that has an obvious fix, you can submit a pull request with a patch in the usual way.</p> <p>If you don't have a fix, you can add an empty comment to the relevant line, and create a pull request from that as a patch. The relevant line(s) will then light up in the pull request's <code>Files changed</code> overview, and you can add your comments there. In this case, either the pull request is never merged (but the comments processed some other way, or not at all), or the extra comments are reverted and replaced by an agreed-upon fix.</p> <p>In all cases, file many small pull requests, not one big one, as GitHub's support for code reviews is rather limited. Putting too many issues into a single pull request quickly becomes unwieldy.</p>"},{"location":"best_practices/code_review/#reviewing-jupyter-notebooks-with-reviewnb","title":"Reviewing Jupyter notebooks with ReviewNB","text":"<p>For rich diffs and commenting on Jupyter notebooks Github is not very useful, because Github shows differences between Jupyter notebooks as a diff between json files. You can use ReviewNB for reviewing Jupyter notebooks. ReviewNB shows the diff between notebooks in a human readable way.  If you configure your esciencecenter.nl email address as your primary email for your Github account here you have access to the academic plan with full benefits.</p>"},{"location":"best_practices/code_review/#references","title":"References","text":"<p>Atwood, Jeff (2006) Code Reviews: Just Do It</p> <p>Burke, Kevin (2011) Why code review beats testing: evidence from decades of programming research.</p> <p>McConnell, Steve (2004) Code Complete: A Practical Handbook of Software Construction, Second Edition. Microsoft Press. ISBN-13: 978-0735619678</p> <p>SMARTBEAR (2016) Best practices for code review.</p>"},{"location":"best_practices/documentation/","title":"Documentation","text":"<p>Developed programs should be documented at multiple levels, from code comments, through API documentation, to installation and usage documentation. Comments at each level should take into account different target audience, from experienced developers, to end users with no programming skills.</p> <p>Example of good documentation: A Guide to NumPy/SciPy Documentation</p>"},{"location":"best_practices/documentation/#markdown","title":"Markdown","text":"<p>Markdown is a lightweight markup language that allows you to create webpages, wikis and user documentation with a minimum of effort. Documentation written in markdown looks exactly like a plain-text document and is perfectly human-readable. In addition, it can also be automatically converted to HTML, latex, pdf, etc. More information about markdown can be found here:</p> <p>http://daringfireball.net/projects/markdown/</p> <p>http://en.wikipedia.org/wiki/Markdown</p> <p>Retext is a markdown aware text editor, that can be used to edit markdown files and convert them into HTML or PDF. It can be found at:</p> <p>https://github.com/retext-project/retext</p> <p>Alternatively, 'pandoc' is a command line utility that can convert markdown documents to into several other formats (including latex):</p> <p>http://johnmacfarlane.net/pandoc/</p> <p>An Eclipse plugin for previewing the HTML generated by markdown is available on this page:</p> <p>https://marketplace.eclipse.org/content/markdown-text-editor</p>"},{"location":"best_practices/documentation/#readme","title":"Readme","text":"<p>Clear explanation of the goal of the project with pointers to other documentation resources.</p> <p>Use GitHub flavoured markdown for, e.g., syntax highlighting. (If reStructuredText or another format that GitHub renders is idiomatic in your community, use that instead.) README is targeted towards developers, it is more technical than home page. Keeping basic documentation in README.md can be even useful for lead developer, to track steps and design decisions. Therefore it is convenient to create it from the beginning of the project, when initialising git repository.</p> <ul> <li>StackOverflow on good readme</li> <li>short gist with README.md template</li> <li>The art of README from nodejs community</li> </ul>"},{"location":"best_practices/documentation/#well-defined-functionality","title":"Well defined functionality","text":"<p>Ideally in README.md</p>"},{"location":"best_practices/documentation/#source-code-documentation","title":"Source code documentation","text":""},{"location":"best_practices/documentation/#code-comments","title":"Code comments","text":"<p>Code comments, can be block comments or inline comments. They are used to explain what is the piece of code doing. Those should explain why something is done in the domain language and not programming language - why instead of what.</p>"},{"location":"best_practices/documentation/#api-documention","title":"API documention","text":"<p>API documentation should explain function arguments and outputs, or the object methods. How they are formulated will depend on the language.</p>"},{"location":"best_practices/documentation/#usage-documentation","title":"Usage documentation","text":"<ul> <li>User manual (as PDF) in the \"doc\" directory. This is the real manual, targeted at your users. Make sure this is readable by domain experts, and not only software developers. Make sure to include:<ul> <li>Netherlands eScience Center logo.</li> <li>Examples.</li> <li>Author name(s).</li> <li>Versions numbers of the software and documentation.</li> <li>References to:<ul> <li>The eScience Center web site.</li> <li>The project web site.</li> <li>The Github page of the project.</li> <li>Location of the issue tracker.</li> <li>More information (e.g. research papers).</li> </ul> </li> </ul> </li> </ul>"},{"location":"best_practices/documentation/#documented-development-setup","title":"Documented development setup","text":"<p>(good example is Getting started with khmer development) It should be made available once there is more than one developer working on the codebase. If your development setup is very complicated, please consider providing a Dockerfile and docker image.</p>"},{"location":"best_practices/documentation/#contribution-guidelines","title":"Contribution guidelines","text":"<p>Contribution guidelines make it easier for collaborators to contribute, and smooth the process of collaboration.</p> <p>Guidelines should be made available once the code is available online and there is a process for contributions by other people. Good guidelines will save time of both lead developer and contributor since things have to be explained only once. A good CONTRIBUTING.md file describes at least how to perform the following tasks:</p> <ul> <li>How to install the dependencies</li> <li>How to run (unit) tests</li> <li>What code style to use</li> <li>Reference to code of conduct</li> <li>Mention the git branching model when using anything else than the GitHub flow branching model</li> </ul> <p>An extensive example is Angular.js's CONTRIBUTING.md. Note that GitHub has built in support for a CONTRIBUTING.md file.</p>"},{"location":"best_practices/documentation/#code-of-conduct","title":"Code of conduct","text":"<p>A code of conduct is a set of rules outlining the social norms, religious rules and responsibilities of, and or proper practices for an individual. Such a document is advantagous for collaboration, for several reasons:</p> <ul> <li>It shows your intent to work together in a positive way with everyone.</li> <li>It reminds everyone to communicate in a welcoming and inclusive way.</li> <li>It provides a set of guidelines in case of conflict.</li> </ul> <p>contributor covenant</p> <p>CofC should be attached from the beginning of the project. There is no gain from having it with one developer, but it does not cost anything to include it in the project and will be handy when more developers join.</p>"},{"location":"best_practices/documentation/#documented-code-style","title":"Documented code style","text":"<p>From the beginning of the project, a decision on the code style has to be made and then should be documented. Not having a documented code style will highly increase the chance of inconsistent style across the codebase, even when only one developer writes code. The Netherlands eScience Center should have a sane suggestion of coding style for each programming language we use. Coding styles are about consistency and making a choice, and not so much about the superiority of one style over the other. A sane set of guides can be found on in google documentation.</p>"},{"location":"best_practices/documentation/#how-to-file-a-bug-report","title":"How to file a bug report","text":"<p>Describing how to properly report a bug will save a lot of developers's time. It is also useful to point users to good bug report guide like one from Simon Tatham</p> <ul> <li>An example of such a document for Mozilla projects</li> <li>Other example from Ubuntu Docuementation</li> </ul>"},{"location":"best_practices/documentation/#explained-meaning-of-issue-labels","title":"Explained meaning of issue labels","text":"<p>Once users start submitting issues labels should be documented.</p>"},{"location":"best_practices/documentation/#doi-or-pid","title":"DOI or PID","text":"<p>making your code citable</p> <p>Identifiers should be associated with releases and should be created together with first release.</p>"},{"location":"best_practices/documentation/#software-citation","title":"Software citation","text":"<p>To get credit for your work, it should be as easy as possible to cite your software.</p> <p>Your software should contain sufficient information for others to be able to cite your software, such as: authors, title, version, journal article (if there is one) and DOI (as described in the DOI section). It is recommended that this information is contained on a single file.</p> <p>You can use the Citation File Format to provide this information on a human- and machine-readable format.</p> <p>Read more in the blog post by Druskat et al..</p>"},{"location":"best_practices/documentation/#print-software-version","title":"Print software version","text":"<p>Make it easy to see which version of the software is in use.</p> <ul> <li>if it's a command line tool: print version on the command line</li> <li>if it's a website: print version within the interface</li> <li>if the tool generates the output: output file should contain the version of software that generated the output</li> </ul>"},{"location":"best_practices/overview/","title":"Software Development","text":"<p>In this chapter we give an overview of the best practices for software development at the Netherlands eScience Center, including a rationale.</p>"},{"location":"best_practices/overview/#know-your-tools","title":"Know your tools","text":"<p>In addition to the advice on the best practices in these chapters, knowing the tools that are available for software development can really help you getting things done faster.</p>"},{"location":"best_practices/overview/#learn-how-to-use-the-command-line-efficiently","title":"Learn how to use the command line efficiently","text":"<p>Read the chapter on using Bash.</p>"},{"location":"best_practices/overview/#use-an-editor-that-helps-you-develop","title":"Use an editor that helps you develop","text":"<p>Commonly used editors and their ecosystem of plugins can really help you write better code faster. Note that for each of the editors and environments listed below, it is important to configure them such that they support the programming languages that you are developing in.</p> <p>Below is a list of editors that support many programming languages.</p> <p>Integrated Development Environments (IDEs): - Visual Studio Code - modern IDE - Atom - modern IDE - Eclipse - a bit older but still nice</p> <p>Text editors: - Sublime Text - modern text editor - vim - classic text editor - emacs - classic text editor</p>"},{"location":"best_practices/releases/","title":"Releases","text":""},{"location":"best_practices/releases/#release","title":"Release","text":"<p>Releases are a way to mark or point to a particular milestone in software development. This is useful for users and collaborators, e.g. I found a bug running version x. For publications that refer to software, refering to a specific release enhances the reproducability.</p> <p>Apache foundation describes their release policy.</p> <p>Release cycles will depend on the project specifics, but in general we encourage quick agile development: release early and often</p>"},{"location":"best_practices/releases/#semantic-versioning","title":"Semantic versioning","text":"<p>Releases are identified by a version number. Semantic Versioning (semver) is the most accepted and used way to add numbers to software versions. It is a way of communicating impact of changes in the software on users.</p> <p>A version number consists of three numbers: major, minor, and patch, separated by a dot: 2.0.0. After some changes to the code, you would do a new release, and increment the version number. Increment the: * MAJOR version when you make incompatible API changes, * MINOR version when you add functionality in a backwards-compatible manner, and * PATCH version when you make backwards-compatible bug fixes.</p> <p>Very often package managers depend on <code>semver</code> and will not work as expected otherwise.</p>"},{"location":"best_practices/releases/#releasing-code-on-github","title":"Releasing code on github","text":"<p>Github makes it easy to do a release straight from your repositories website. See github releases for more information.</p>"},{"location":"best_practices/releases/#changelogmd","title":"CHANGELOG.md","text":"<p>A change log is a way to communicate notable changes in a release to the users and contributors. It is typically a text file at the root of your repository called CHANGELOG.md. Every release should have relevant entry in change log.</p> <p>See Keep a CHANGELOG for some best practices.</p>"},{"location":"best_practices/releases/#one-command-install","title":"One command install","text":"<p>To not scare away users and (potential) collaborators, installing the software should be easy, a one command process. The process itself typically includes installing dependencies, compiling, testing, and finally actual installation, and can be quite complex. The use of a proper build system is strongly recommended.</p>"},{"location":"best_practices/releases/#package-in-package-manager","title":"Package in package manager","text":"<p>If your software is useful for a wider audience, create a package that can be installed with a package manager. Package managers can also be used to install dependencies quickly and easily. * For Python use pip * For Javascript use npm * C, C++, Fortran, ... use packages from your distributions official repository. List your actual dependencies in the INSTALL.md or README.md</p> <p>Some standard solutions for building (compiling) code are: * The Autotools: autoconf, automake, and libtool. See the Autotools Documentation, or an introductionary presentation by Thomas Petazzoni * CMake * Make</p>"},{"location":"best_practices/releases/#release-quick-scan-by-other-engineer","title":"Release quick-scan by other engineer","text":"<p>A check by a fellow engineer to see if the documentation is understandable? can the software be installed? etc.</p> <p>Think of it as a kind of code review but with focus on mechanics, not code. The reviewer should check if: (i) there is easily visible or findable documentation, (ii) download works, (iii) there are instructions on how to (iv) install and (v) start using software, some of the things in this scan could be automated with continuous integration.</p>"},{"location":"best_practices/releases/#citeable","title":"Citeable","text":"<p>Create a DOI for each release see Making software citable.</p>"},{"location":"best_practices/releases/#dissemination","title":"Dissemination","text":"<p>When you have a first stable release, or a subsequent major releases, let the world know! Inform your coordinator and our Communications Advisor so we can write news item on our site, add it to the annual report, etc.</p>"},{"location":"best_practices/standards/","title":"Use standards","text":"<p>Standard files and protocols should always be a primary choice. Using standards improves the interoperability of your software, thereby improving its usefulness.</p>"},{"location":"best_practices/standards/#exchange-formats","title":"Exchange formats","text":"<p>Examples include Unicode W3C, OGN, NetCDF, etc.</p>"},{"location":"best_practices/standards/#protocols","title":"Protocols","text":"<p>Examples include HTTP, TCP, TLS, etc.</p>"},{"location":"language_guides/bash/","title":"Bash","text":"<p>Page maintainer: Bouwe Andela @bouweandela</p> <p>Bash is both a command line interface, also known as a shell, and a scripting language. On most Linux distributions, the Bash shell is the default way of interacting with the system. Zsh is an alternative shell that also understands the Bash scripting language, this is the default shell on recent versions of Mac OS. Both Bash and Zsh are available for most operating systems.</p> <p>At the Netherlands eScience Center, Bash is the recommended shell scripting language because it is the most commonly used shell language and therefore the most convenient for collaboration. To facilitate mutual understanding, it is also recommended that you are aware of the shell that your collaborators are using and that you write documentation with this in mind. Using the same shell as your collaborators is a simple way of making sure you are always on the same page.</p> <p>In this chapter, a short introduction and best practices for both interactive and use in scripts will be given. An excellent tutorial introducing Bash can be found here. If you have not used Bash or another shell before, it is recommended that you follow the tutorial before continuing reading. Learning to use Bash is highly recommended, because after some initial learning, you will be more efficient and have a better understanding of what is going on than when clicking buttons from the graphical user interface of your operating system or integrated development environment.</p>"},{"location":"language_guides/bash/#interactive-use","title":"Interactive use","text":"<p>If you are a (research) software engineer, it is highly recommended that you learn</p> <ul> <li>the keyboard shortcuts</li> <li>how to configure Bash aliases</li> <li>the name and function of commonly used command line tools</li> </ul>"},{"location":"language_guides/bash/#bash-keyboard-shortcuts","title":"Bash keyboard shortcuts","text":"<p>An introduction to bash keyboard shortcuts can be found here. Note that Bash can also be configured such that it uses the vi keyboard shortcuts instead of the default emacs ones, which can be useful if you prefer vi.</p>"},{"location":"language_guides/bash/#bash-aliases","title":"Bash aliases","text":"<p>Bash aliases allow you to define shorthands for commands you use often. Typically these are defined in the <code>~/.bashrc</code> or <code>~/.bash_aliases</code> file.</p>"},{"location":"language_guides/bash/#commonly-used-command-line-tools","title":"Commonly used command line tools","text":"<p>It is recommended that you know at least the names and use of the following command line tools. The details of how to use a tool exactly can easily be found by searching the internet or using <code>man</code> to read the manual, but you will be vastly more efficient if you already know the name of the command you are looking for.</p> <p>Working with files</p> <ul> <li><code>ls</code> - List files and directories</li> <li><code>tree</code> - Graphical representation of a directory structure</li> <li><code>cd</code> - Change working directory</li> <li><code>pwd</code> - Show current working directory</li> <li><code>cp</code> - Copy a file or directory</li> <li><code>mv</code> - Move a file or directory</li> <li><code>rm</code> - Remove a file or directory</li> <li><code>mkdir</code> - Make a new directory</li> <li><code>touch</code> - Make a new empty file or update its access and modification time to the current time</li> <li><code>chmod</code> - Change the permissions on a file or directory</li> <li><code>chown</code> - Change the owner of a file or directory</li> <li><code>find</code> - Search for files and directories on the file system</li> <li><code>locate</code>, <code>updatedb</code> - Search for files and directories quickly using a database</li> <li><code>tar</code> - (Un)pack .tar or .tar.gz files</li> <li><code>unzip</code> - Unpack .zip files</li> <li><code>df</code>, <code>du</code> - Show free space on disk, show disk space usage of files/folders</li> </ul> <p>Working with text</p> <p>Here we list the most commonly used Bash tools that are built to manipulate lines of text. The nice thing about these tools is that you can combine them by streaming the output of one tool to become the input of the next tool. Have a look at the tutorial for an introduction. This can be done by creating pipelines with the pipe operator <code>|</code> and by redirecting text to output streams or files using redirection operators like <code>&gt;</code> for output and <code>&lt;</code> for input to a command from a text file.</p> <ul> <li><code>echo</code> - Repeat some text</li> <li><code>diff</code> - Show the difference between two text files</li> <li><code>grep</code> - Search for lines of text matching a simple string or regular expressions</li> <li><code>sed</code> - Edit lines of text using regular expressions</li> <li><code>cut</code> - Select columns from text</li> <li><code>cat</code> - Print the content of a file</li> <li><code>head</code> - Print the first n lines</li> <li><code>tail</code> - Print the last n lines</li> <li><code>tee</code> - Read from standard input and write to standard output and file</li> <li><code>less</code> - Read text</li> <li><code>sort</code> - Sort lines of text</li> <li><code>uniq</code> - Keep unique lines</li> <li><code>wc</code> - Count words/lines</li> <li><code>nano</code>, <code>emacs</code>, <code>vi</code> - Interactive text editors found on most Unix systems</li> </ul> <p>Working with programs</p> <ul> <li><code>man</code> - Read the manual</li> <li><code>ps</code> - Print all currently running programs</li> <li><code>top</code> - Interactively display all currently running programs</li> <li><code>kill</code> - Stop a running program</li> <li><code>\\time</code> - Collect statistics about resource usage such as runtime, memory use, storage access (the <code>\\</code> in front is needed to run the <code>time</code> program instead of the bash builtin function with the same name)</li> <li><code>which</code> - Find which file will be executed when you run a command</li> <li><code>xargs</code> - Run programs with arguments in parallel</li> </ul> <p>Working with remote systems</p> <ul> <li><code>ssh</code> - Connect to a shell on a remote computer</li> <li><code>rsync</code> - Copy files between computers using SSH/SFTP</li> <li><code>lftp</code> - Copy files between computers using FTP</li> <li><code>wget</code>, <code>curl</code> - Copy a file using https or make a request to a remote API</li> <li><code>scp</code>, <code>sftp</code>, <code>ftp</code> - Simple tools for transferring files over (S)FTP - not recommended</li> <li><code>who</code> - show who is logged on</li> <li><code>screen</code> - Run multiple bash sessions and keep them running even when you log out</li> </ul> <p>Installing software</p> <ul> <li><code>apt</code> - The default package manager on Debian based Linux distributions</li> <li><code>yum</code>, <code>dnf</code> - The default package manager on RedHat/Fedora based Linux distributions</li> <li><code>brew</code> - A package manager for MacOS</li> <li><code>conda</code> - A package manager that supports many operating systems</li> <li><code>pip</code> - The Python package manager</li> <li><code>docker</code>, <code>singularity</code> - Run an entire Linux operating system including software from a container</li> </ul> <p>Miscellaneous</p> <ul> <li><code>bash</code>, <code>zsh</code> - The command to start Bash/Zsh</li> <li><code>history</code> - View all past commands</li> <li><code>fg</code>, <code>bg</code> - Move a program to the foreground, background, useful with Ctrl+Z</li> <li><code>su</code> - Switch user</li> <li><code>sudo</code> - Run a command with root permissions</li> </ul> <p>For further inspiration, see this extensive list of command line tools.</p>"},{"location":"language_guides/bash/#scripts","title":"Scripts","text":"<p>It is possible to write bash scripts. This is done by writing the commands that you would normally use on the command line in text file and e.g. running the file with <code>bash some-file.sh</code>.</p> <p>However, doing this is only recommended if there really are no other options. If you have the option to write a Python script instead, that is the recommended way to go. This will bring you all the advantages of a fully-fledged programming language (such as libraries, frameworks for testing and documentation) and Python is the recommended programming language at the Netherlands eScience Center. If you do not mind having an extra dependency and would like to use the features and commands available in the shell from Python, the sh library is a nice option.</p> <p>Disclaimer: if you are an experienced Bash developer, there might be situations where using a Bash script solves your problem faster or in a more portable way than a Python script. Do take take a moment to think about whether such a solution is easy to contribute to for collaborators and will be easy to maintain in the future, as the number of features, supported systems, and code paths grows.</p> <p>When writing a bash script, always use <code>shellcheck</code> to make sure that your bash script is as likely to do what you think it should do as possible.</p> <p>In addition to that, always start the script with <pre><code>set -euo pipefail\n</code></pre> this will stop the script if there is</p> <ul> <li><code>-e</code> a command that exits with a non-zero exit code</li> <li><code>-o pipefail</code> a command in a pipe that exits with a non-zero exit code</li> <li><code>-u</code> an undefined variable in your script</li> </ul> <p>an exit code other than zero usually indicates that an error occurred. If needed, you can temporarily allow this kind of error for a single line by wrapping it like this <pre><code>set +e\nfalse  # A command that returns a non-zero exit code\nset -e\n</code></pre></p>"},{"location":"language_guides/bash/#further-resources","title":"Further resources","text":"<ul> <li>Bash Tutorial</li> <li>Bash Cheat sheet</li> <li>The Bash Reference Manual or use <code>man bash</code></li> <li>Oh My Zsh offers an extensive set of themes and shortcuts for the Zsh</li> </ul>"},{"location":"language_guides/ccpp/","title":"C and C++","text":"<p>Page maintainer: Johan Hidding @jhidding</p> <p>C++ is one of the hardest languages to learn. Entering a project where C++ coding is needed should not be taken lightly. This guide focusses on tools and documentation for use of C++ in an open-source environment.</p>"},{"location":"language_guides/ccpp/#standards","title":"Standards","text":"<p>The latest ratified standard of C++ is C++17. The first standardised version of C++ is from 1998. The next version of C++ is scheduled for 2020. With these updates (especially the 2011 one) the preferred style of C++ changed drastically. As a result, a program written in 1998 looks very different from one from 2018, but it still compiles. There are many videos on Youtube describing some of these changes and how they can be used to make your code look better (i.e. more maintainable). This goes with a warning: Don't try to be too smart; other people still have to understand your code.</p>"},{"location":"language_guides/ccpp/#practical-use","title":"Practical use","text":""},{"location":"language_guides/ccpp/#compilers","title":"Compilers","text":"<p>There are two main-stream open-source C++ compilers.</p> <ul> <li>GCC</li> <li>LLVM - CLANG</li> </ul> <p>Overall, these compilers are more or less similar in terms of features, language support, compile times and (perhaps most importantly) performance of the generated binaries. The generated binary performance does differ for specific algorithms. See for instance this Phoronix benchmark for a comparison of GCC 9 and Clang 7/8.</p> <p>MacOS (XCode) has a custom branch of <code>clang</code>, which misses some features like OpenMP support, and its own libcxx, which misses some standard library things like the very useful <code>std::filesystem</code> module. It is nevertheless recommended to use it as much as possible to maintain binary compatibility with the rest of macOS.</p> <p>If you need every last erg of performance, some cluster environments have the Intel compiler installed.</p> <p>These compilers come with a lot of options. Some basic literacy in GCC and CLANG:</p> <ul> <li><code>-O</code> changes optimisation levels</li> <li><code>-std=c++xx</code> sets the C++ standard used</li> <li><code>-I*path*</code> add path to search for include files</li> <li><code>-o*file*</code> output file</li> <li><code>-c</code> only compile, do not link</li> <li><code>-Wall</code> be more verbose with warnings</li> </ul> <p>And linker flags:</p> <ul> <li><code>-l*library*</code> links to a library</li> <li><code>-L*path*</code> add path to search for libraries</li> <li><code>-shared</code> make a shared library</li> <li><code>-Wl,-z,defs</code> ensures all symbols are accounted for when linking to a shared object</li> </ul>"},{"location":"language_guides/ccpp/#interpreter","title":"Interpreter","text":"<p>There is a C++ interpreter called Cling. This also comes with a Jupyter notebook kernel.</p>"},{"location":"language_guides/ccpp/#build-systems","title":"Build systems","text":"<p>There are several build systems that handle C/C++. Currently, the CMake system is most popular. It is not actually a build system itself; it generates build files based on (in theory) platform-independent and compiler-independent configuration files. It can generate Makefiles, but also Ninja files, which gives much faster build times, NMake files for Windows and more. Some popular IDEs keep automatic count for CMake, or are even completely built around it (CLion). The major drawback of CMake is the confusing documentation, but this is generally made up for in terms of community support. When Googling for ways to write your CMake files, make sure you look for \"modern CMake\", which is a style that has been gaining traction in the last few years and makes everything better (e.g. dependency management, but also just the CMake files themselves).</p> <p>Traditionally, the auto-tools suite (AutoConf and AutoMake) was the way to build things on Unix; you'll probably know the three command salute:</p> <pre><code>&gt; ./configure --prefix=~/.local\n    ...\n&gt; make -j4\n    ...\n&gt; make install\n</code></pre> <p>With either one of these two (CMake or Autotools), any moderately experienced user should be able to compile your code (if it compiles).</p> <p>There are many other systems. Microsoft Visual Studio has its own project model / build system and a library like Qt also forces its own build system on you. We do not recommend these if you don't also supply an option for building with CMake or Autotools. Another modern alternative that has been gaining attention mainly in the GNU/Gnome/Linux world is Meson, which is also based on Ninja.</p>"},{"location":"language_guides/ccpp/#package-management","title":"Package management","text":"<p>There is no standard package manager like <code>pip</code>, <code>npm</code> or <code>gem</code> for C++. This means that you will have to choose depending on your particular circumstances what tool to use for installing libraries and, possibly, packaging the tools you yourself built. Some important factors include: - Whether or not you have root/admin access to your system - What kind of environment/ecosystem you are working in. For instance:     * There are many tools targeted specifically at HPC/cluster environments.     * Specific communities (e.g. NLP research or bioinformatics) may have gravitated towards specific tools, so you'll probably want to use those for maximum impact. - Whether software is packaged at all; many C/C++ tools only come in source form, hopefully with build setup configuration.</p>"},{"location":"language_guides/ccpp/#yes-root-access","title":"Yes root access","text":"<p>If you have root/admin access to your system, the first go-to for libraries may be your OS package manager. If the target package is not in there, try to see if there is an equivalent library that is, and see what kind of software uses it.</p>"},{"location":"language_guides/ccpp/#no-root-access","title":"No root access","text":"<p>A good, cross-platform option nowadays is to use <code>miniconda</code>, which works on Linux, macOS and Windows. The <code>conda-forge</code> channel especially has a lot of C++ libraries. Specify that you want to use this channel with command line option <code>-c conda-forge</code>. The <code>bioconda</code> channel in turn builds upon the <code>conda-forge</code> libraries, hosting a lot of bioinformatics tools.</p>"},{"location":"language_guides/ccpp/#managing-non-packaged-software","title":"Managing non-packaged software","text":"<p>If you do have to install a programm, which depends on a specific version of a library which depends on a specific version of another library, you enter what is called dependency hell. Some agility in compiling and installing libraries is essential.</p> <p>You can install libraries in <code>/usr/local</code> or in <code>${HOME}/.local</code> if you aren't root, but there you have no package management.</p> <p>Many HPC administrations provide environment modules (<code>module avail</code>), which allow you to easily populate your <code>$PATH</code> and other environment variables to find the respective package. You can also write your own module files to solve your dependency hell.</p> <p>A lot of libraries come with a package description for <code>pkg-config</code>. These descriptions are installed in <code>/usr/lib/pkgconfig</code>. You can point <code>pkg-config</code> to your additional libraries by setting the <code>PKG_CONFIG_PATH</code> environment variable. This also helps for instance when trying to automatically locate dependencies from CMake, which has <code>pkg-config</code> support as a fallback for when libraries don't support CMake's <code>find_package</code>.</p> <p>If you want to keep things organized on systems where you use multiple versions of the same software for different projects, a simple solution is to use something like <code>xstow</code>. XStow is a poor-mans package manager. You install each library in its own directory (<code>~/.local/pkg/&lt;package&gt;</code> for instance), then running <code>xstow</code> will create symlinks to the files in the <code>~/.local</code> directory (one above the XStow package directory). Using XStow in this way alows you to keep a single additional search path when compiling your next library.</p>"},{"location":"language_guides/ccpp/#packaging-software","title":"Packaging software","text":"<p>In case you find the manual compilation too cumbersome, or want to conveniently distribute software (your own or perhaps one of your project's dependencies that the author did not package themselves), you'll have to build your own package. The above solutions are good defaults for this, but there are some additional options that are widely used. - For distribution to root/admin users: system package managers (Linux: <code>apt</code>, <code>yum</code>, <code>pacman</code>, macOS: Homebrew, Macports) - For distribution to any users: Conda and Conan are cross-platform (Linux, macOS, Windows) - For distribution to HPC/cluster users: see options below</p> <p>When choosing which system to build your package for, it is imporant to consider your target audience. If any of these tools are already widely used in your audience, pick that one. If not, it is really up to your personal preferences, as all tools have their pros and cons. Some general guidelines could be: - prefer multi-platform over single platform - prefer widely used over obscure (even if it's technically magnificent, if nobody uses it, it's useless for distributing your software) - prefer multi-language over single language (especially for C++, because it is so often used to build libraries that power higher level languages)</p> <p>But, as the state of the package management ecosystem shows, in practice, there will be many exceptions to these guidelines.</p>"},{"location":"language_guides/ccpp/#hpccluster-environments","title":"HPC/cluster environments","text":"<p>One way around this if the system does use <code>module</code> is to use Easybuild, which makes installing modules in your home directory quite easy. Many recipes (called Easyblocks) for building packages or whole toolchains are available online. These are written in Python.</p> <p>A similar package that is used a lot in the bioinformatics community is guix. With guix, you can create virtual environments, much like those in Python <code>virtualenv</code> or Conda. You can also create relocatable binaries to use your binaries on systems that do not have guix installed. This makes it easy to test your packages on your laptop before deploying to a cluster system.</p> <p>A package that gains more traction at the moment for HPC environments is spack. Spack allows you to pick from many compilers. When installing packages, it compiles every package from scratch. This allows you to be tailor compilation flags and such to take fullest advantage of your cluster's hardware, which can be essential in HPC situations</p>"},{"location":"language_guides/ccpp/#near-future-modules","title":"Near future: Modules","text":"<p>Note that C++20 will bring Modules, which can be used as an alternative to including (precompiled) header files. This will allow for easier packaging and will probably cause the package management landscape to change considerably. For this reason, it may be wise at this time to keep your options open and keep an eye on developments within the different package management solutions.</p>"},{"location":"language_guides/ccpp/#editors","title":"Editors","text":"<p>This is largely a matter of taste, but not always.</p> <p>In theory, given that there are many good command line tools available for working with C(++) code, any code editor will do to write C(++). Some people also prefer to avoid relying on IDEs too much; by helping your memory they can also help you to write less maintainable code. People of this persuasion would usually recommend any of the following editors: * Vim, recommended plugins:   + NERDTree file explorer.   + editorconfig   + stl.vim adds STL to syntax highlighting   + Syntastic   + Integrated debugging using Clewn * Emacs:   + Has GDB mode for debugging. * More modern editors: Atom / Sublime Text / VS Code   + Rich plugin ecosystem   + Easier on the eyes... I mean modern OS/GUI integration</p> <p>In practice, sometimes you run into large/complex existing projects and navigating these can be really hard, especially when you just start working on the project. In these cases, an IDE can really help. Intelligent code suggestions, easy jumping between code segments in different files, integrated debugging, testing, VCS, etc. can make the learning curve a lot less steep. Good/popular IDEs are * CLion * Visual Studio (Windows only, but many people swear by it) * Eclipse</p>"},{"location":"language_guides/ccpp/#code-and-program-quality-analysis","title":"Code and program quality analysis","text":"<p>C++ (and C) compilers come with built in linters and tools to check that your program runs correctly, make sure you use those. In order to find issues, it is probably a good idea to use both compilers (and maybe the valgrind memcheck tool too), because they tend to detect different problems.</p>"},{"location":"language_guides/ccpp/#automatic-formatting-with-clang-format","title":"Automatic Formatting with clang-format","text":"<p>While most IDEs and some editors offer automatic formatting of files, clang-format is a standalone tool, which offers sensible defaults and a huge range of customisation options. Integrating it into the CI workflow guarantees that checked in code adheres to formatting guidelines.</p>"},{"location":"language_guides/ccpp/#static-code-analysis-with-gcc","title":"Static code analysis with GCC","text":"<p>To use the GCC linter, use the following set of compiler flags when compiling C++ code: <pre><code>-O2 -Wall -Wextra -Wcast-align -Wcast-qual -Wctor-dtor-privacy -Wdisabled-optimization -Wformat=2\n-Winit-self -Wlogical-op -Wmissing-declarations -Wmissing-include-dirs -Wnoexcept -Wold-style-cast\n-Woverloaded-virtual -Wredundant-decls -Wshadow -Wsign-conversion -Wsign-promo -Wstrict-null-sentinel\n-Wstrict-overflow=5 -Wswitch-default -Wundef -Wno-unused\n</code></pre> and these flags when compiling C code: <pre><code>-O2 -Wall -Wextra -Wformat-nonliteral -Wcast-align -Wpointer-arith -Wbad-function-cast\n-Wmissing-prototypes -Wstrict-prototypes -Wmissing-declarations -Winline -Wundef\n-Wnested-externs -Wcast-qual -Wshadow -Wwrite-strings -Wno-unused-parameter\n-Wfloat-equal\n</code></pre> Use at least optimization level 2 (<code>-O2</code>) to have GCC perform code analysis up to a level where you get all warnings. Use the <code>-Werror</code> flag to turn warnings into errors, i.e. your code won't compile if you have warnings. See this post for an explanation of why this is a reasonable selection of warning flags.</p>"},{"location":"language_guides/ccpp/#static-code-analysis-with-clang-llvm","title":"Static code analysis with Clang (LLVM)","text":"<p>Clang has the very convenient flag <pre><code>-Weverything\n</code></pre> A good strategy is probably to start out using this flag and then disable any warnings that you do not find useful.</p>"},{"location":"language_guides/ccpp/#static-code-analysis-with-cppcheck","title":"Static code analysis with cppcheck","text":"<p>An additional good tool that detects many issues is cppcheck. Most editors/IDEs have plugins to use it automatically.</p>"},{"location":"language_guides/ccpp/#dynamic-program-analysis-using-fsanitize","title":"Dynamic program analysis using <code>-fsanitize</code>","text":"<p>Both GCC and Clang allow you to compile your code with the <code>-fsanitize=</code> flag, which will instrument your program to detect various errors quickly. The most useful option is probably <pre><code>-fsanitize=address -O2 -fno-omit-frame-pointer -g\n</code></pre> which is a fast memory error detector. There are also other options available like <code>-fsanitize=thread</code> and <code>-fsanitize=undefined</code>. See the GCC man page or the Clang online manual for more information.</p>"},{"location":"language_guides/ccpp/#dynamic-program-analysis-using-the-valgrind-suite-of-tools","title":"Dynamic program analysis using the valgrind suite of tools","text":"<p>The valgrind suite of tools has tools similar to what is provided by the <code>-fsanitize</code> compiler flag as well as various profiling tools. Using the valgrind tool memcheck to detect memory errors is typically slower than using compiler provided option, so this might be something you will want to do less often. You will probably want to compile your code with debug symbols enabled (<code>-g</code>) in order to get useful output with memcheck. When using the profilers, keep in mind that a statistical  profiler may give you more realistic results.</p>"},{"location":"language_guides/ccpp/#automated-code-refactoring","title":"Automated code refactoring","text":"<p>Sometimes you have to update large parts of your code base a little bit, like when you move from one standard to another or you changed a function definition. Although this can be accomplished with a <code>sed</code> command using regular expressions, this approach is dangerous, if you use macros, your code is not formatted properly etc.... Clang-tidy can do these things and many more by using the abstract syntax tree of the compiler instead of the source code files to refactor your code and thus is much more robust but also powerful.</p>"},{"location":"language_guides/ccpp/#debugging","title":"Debugging","text":"<p>Most of your time programming C(++) will probably be spent on debugging. At some point, surrounding every line of your code with <code>printf(\"here %d\", i++);</code> will no longer avail you and you will need a more powerful tool. With a debugger, you can inspect the program while it is running. You can pause it, either at random points when you feel like it or, more usually, at so-called breakpoints that you specified in advance, for instance at a certain line in your code, or when a certain function is called. When paused, you can inspect the current values of variables, manually step forward in the code line by line (or by function, or to the next breakpoint) and even change values and continue running. Learning to use these powerful tools is a very good time investment. There are some really good CppCon videos about debugging on YouTube.</p> <ul> <li>GDB - the GNU Debugger, many graphical front-ends are based on GDB.</li> <li>LLDB - the LLVM debugger. This is the go-to GDB alternative for the LLVM toolchain, especially on macOS where GDB is hard to setup.</li> <li>DDD - primitive GUI frontend for GDB.</li> <li>The IDEs mentioned above either have custom built-in debuggers or provide an interface to GDB or LLDB.</li> </ul>"},{"location":"language_guides/ccpp/#libraries","title":"Libraries","text":"<p>Historically, many C and C++ projects have seemed rather hestitant about using external dependencies (perhaps due to the poor dependency management situation mentioned above). However, many good (scientific) computing libraries are available today that you should consider using if applicable. Here follows a list of libraries that we recommend and/or have experience with. These can typically be installed from a wide range of package managers.</p>"},{"location":"language_guides/ccpp/#usual-suspects","title":"Usual suspects","text":"<p>These scientific libraries are well known, widely used and have a lot of good online documentation.</p> <ul> <li>GNU Scientific library (GSL)</li> <li>FFTW: Fastest Fourier Transform in the West</li> <li>OpenMPI. Use with caution, since it will strongly define the structure of your code, which may or may not be desirable.</li> </ul>"},{"location":"language_guides/ccpp/#boost","title":"Boost","text":"<p>This is what the Google style guide has to say about Boost:</p> <ul> <li>Definition: The Boost library collection is a popular collection of peer-reviewed, free, open-source C++ libraries.</li> <li>Pros: Boost code is generally very high-quality, is widely portable, and fills many important gaps in the C++ standard library, such as type traits and better binders.</li> <li>Cons: Some Boost libraries encourage coding practices which can hamper readability, such as metaprogramming and other advanced template techniques, and an excessively \"functional\" style of programming.</li> </ul> <p>As a general rule, don't use Boost when there is equivalent STL functionality.</p>"},{"location":"language_guides/ccpp/#xtensor","title":"xtensor","text":"<p>xtensor is a modern (C++14) N-dimensional tensor (array, matrix, etc) library for numerical work in the style of Python's NumPy. It aims for maximum performance (and in most cases it succeeds) and has an active development community. This library features, among other things: * Lazy-evaluation: only calculate when necessary. * Extensible template expressions: automatically optimize many subsequent operations into one \"kernel\". * NumPy style syntax, including broadcasting. * C++ STL style interfaces for easy integration with STL functionality. * Very low-effort integration with today's main data science languages Python, R and Julia. This all makes xtensor a very interesting choice compared to similar older libraries like Eigen and Armadillo.</p>"},{"location":"language_guides/ccpp/#general-purpose-io","title":"General purpose, I/O","text":"<ul> <li>Configuration file reading and writing:<ul> <li>yaml-cpp: A YAML parser and emitter in C++</li> <li>JSON for Modern C++</li> </ul> </li> <li>Command line argument parsing:<ul> <li>argagg</li> <li>Clara</li> </ul> </li> <li>fmt: pythonic string formatting</li> <li>hdf5: The popular HDF5 binary format C++ interface.</li> </ul>"},{"location":"language_guides/ccpp/#parallel-processing","title":"Parallel processing","text":"<ul> <li>oneAPI Threading Building Blocks (oneTBB): template library for task parallelism</li> <li>ZeroMQ: lower level flexible communication library with a unified interface for message passing between threads and processes, but also between separate machines via TCP.</li> </ul>"},{"location":"language_guides/ccpp/#style","title":"Style","text":""},{"location":"language_guides/ccpp/#style-guides","title":"Style guides","text":"<p>Good style is not just about layout and linting on trailing whitespace. It will mean the difference between a blazing fast code and a broken one.</p> <ul> <li>C++ Core Guidelines</li> <li>Guidelines Support Library</li> <li>Google Style Guide</li> <li>Google Style Guide - github Contains the CppLint linter.</li> </ul>"},{"location":"language_guides/ccpp/#project-layout","title":"Project layout","text":"<p>A C++ project will usually have directories <code>/src</code> for source codes, <code>/doc</code> for Doxygen output, <code>/test</code> for testing code. Some people like to put header files in <code>/include</code>. In C++ though, many header files will contain functioning code (templates and inline functions). This makes the separation between code and interface a bit murky. In this case, it can make more sense to put headers and implementation in the same tree, but different communities will have different opinions on this. A third option that is sometimes used is to make separate \"template implementation\" header files.</p>"},{"location":"language_guides/ccpp/#sustainability","title":"Sustainability","text":""},{"location":"language_guides/ccpp/#testing","title":"Testing","text":"<p>Use Google Test. It is light-weight, good and is used a lot. Catch2 is also pretty good, well maintained and has native support in the CLion IDE.</p>"},{"location":"language_guides/ccpp/#documentation","title":"Documentation","text":"<p>Use Doxygen. It is the de-facto standard way of inlining documentation into comment sections of your code. The output is very ugly. Mini-tutorial: run <code>doxygen -g</code> (preferably inside a <code>doc</code> folder) in a new project to set things up, from then on, run <code>doxygen</code> to (re-)generate the documentation.</p> <p>A newer but less mature option is cldoc.</p>"},{"location":"language_guides/ccpp/#resources","title":"Resources","text":""},{"location":"language_guides/ccpp/#online","title":"Online","text":"<ul> <li>CppCon videos: Many really good talks recorded at the various CppCon meetings.</li> <li>CppReference.com</li> <li>C++ Annotations</li> <li>CPlusPlus.com</li> <li>Modern C++, according to Microsoft</li> </ul>"},{"location":"language_guides/ccpp/#books","title":"Books","text":"<ul> <li>Bjarne Soustrup - The C++ Language</li> <li>Scott Meyers - Effective Modern C++</li> </ul>"},{"location":"language_guides/fortran/","title":"Fortran","text":"<p>Page maintainer: Gijs van den Oord @goord</p> <p>Disclaimer: In general the Netherlands eScience Center does not recommend using Fortran. However, in some cases it is the only viable option, for instance if a project builds upon existing code written in this language. This section will be restricted to Fortran90, which captures majority of Fortran source code.</p> <p>The second use case may be extremely performance-critical dense numerical compute workloads, with no existing alternative. In this case it is recommended to keep the Fortran part of the application minimal, using a high-level language like Python for program control flow, IO, and user interface.</p>"},{"location":"language_guides/fortran/#recommended-sources-of-information","title":"Recommended sources of information","text":"<ul> <li>Fortran90 official documentation</li> <li>Fortran wiki</li> <li>Fortran90 handbook</li> </ul>"},{"location":"language_guides/fortran/#compilers","title":"Compilers","text":"<ul> <li>gfortran: the official GNU Fortran compiler and part of the gcc compiler suite.</li> <li>ifort: the Intel Fortran compiler, widely used in academia and industry because of its superior performance, but   unfortunately this is commercial software so not recommended. The same holds for the Portland compiler pgfortran</li> </ul>"},{"location":"language_guides/fortran/#debuggers-and-diagnostic-tools","title":"Debuggers and diagnostic tools","text":"<p>There exist many commercial performance profiling tools by Intel and the Portland Group which we shall not discuss here. Most important freely available alternatives are * gdb: the GNU debugger, part of the gcc compiler suite. Use the -g option to compile with debugging symbols. * gprof: the GNU profiler, part of gcc too. Use the -p option to compile with profiling enabled. * valgrind: to detect memory leaks.</p>"},{"location":"language_guides/fortran/#editors-and-ides","title":"Editors and IDEs","text":"<p>Most lightweight editors provide Fortran syntax highlighting. Vim and emacs are most widely used, but for code completion and refactoring tools one might consider the CBFortran distribution of Code::Blocks.</p>"},{"location":"language_guides/fortran/#coding-style-conventions","title":"Coding style conventions","text":"<p>If working on an existing code base, adopt the existing conventions. Otherwise we recommend the standard conventions, described in the official documentation and the Fortran company style guide. We would like to add the following advice:</p> <ul> <li>Use free-form text input style (the default), with a maximal line width well below the 132 characters imposed by the Fortran90 standard.</li> <li>When a method does not need to alter any data in any module and returns a single value, use a function for it, otherwise use a subroutine. Minimize the latter to reasonable extent.</li> <li>Use the intent attributes in subroutine variable declarations as it makes the code much easier to understand.</li> <li>Use a performance-driven approach to the architecture, do not use the object-oriented features of Fortran90 if they slow down execution. Encapsulation by modules is perfectly acceptable.</li> <li>Add concise comments to modules and routines, and add comments to less obvious lines of code.</li> <li>Provide a test suite with your code, containing both unit and integration tests. Both automake and cmake provide test   suite functionality; if you create your makefile yourself, add a separate testing target.</li> </ul>"},{"location":"language_guides/javascript/","title":"Getting started","text":"<p>Page maintainer: Ewan Cahen @ewan-escience</p> <p>JavaScript (JS) is a programming language that is one of the three (together with HTML and CSS) core technologies of the web. It is essential if you want to write interactive webpages or web applications, because JavaScript is, apart from WebAssembly, the only programming language that runs in modern browsers. Furthermore, JS can also run outside of the browser, e.g. for running short scripts or full-blown servers.</p> <p>A good introductory tutorial on JavaScript is this one from W3Schools.</p> <p>Another source of information for JavaScript (and web development in general) is the MDN Web Docs.</p>"},{"location":"language_guides/javascript/#frameworks","title":"Frameworks","text":"<p>Many people will jump straight to using a framework when building a web application. We, however, recommend that you learn the fundamentals first and get an impression of what problems frameworks are trying to solve for you. Read, for example, this article on how the web works a look at this introduction to the DOM.</p> <p>A good video summary on the history of frameworks and the problems they try to solve can be found here.</p> <p>Before you pick a framework, you should first consider what you are trying to build.</p> <ul> <li>If you're building a (more traditional) website with mostly static content, like an info page for an event or a blog, whose content doesn't adapt to the visitor, consider using a static site generator like Jekyll or Hugo or Docusaurus for writing documentation. An advantage of this is that static sites can be hosted on GitHub for free, which uses Jekyll by default (but you can use other static site generators as well).</li> <li>If you're building a website that is not very interactive, but that many people have to edit, and when a static site generator is too technical, consider using WordPress. Many hosting providers support WordPress out of the box.</li> <li>When you need light interactivity, the options above can be combined with libraries like jQuery, Alpine.js, htmx or you can write the JavaScript yourself.</li> <li>When you want to build a website that has high interactivity with its users, something you would call an \"application\" rather than a \"website\", consider using htmx or one of the JavaScript frameworks below.</li> </ul> <p>Currently, the most popular frameworks are (ordered by popularity according to the StackOverflow 2024 Developer Survey)</p> <ul> <li>React</li> <li>Angular</li> <li>Vue.js</li> <li>Svelte</li> <li>SolidJS</li> </ul>"},{"location":"language_guides/javascript/#react","title":"React","text":"<p>React is a framework which can used to create interactive User Interfaces by combining components. It is developed by Facebook. It is by far the most popular framework, resulting in a huge choice of libraries and a lot of available documentation. Contrary to most other frameworks, React apps are typically written in JSX instead of plain HTML, CSS and JS.</p> <p>Where other frameworks like Angular and Vue.js include rendering, routing and, state management functionality, React only does rendering, so other libraries must be used for routing and state management. Redux can be used to let state changes flow through React components. React Router can be used to navigate the application using URLs. Or you can use a so-called \"meta-framework\" like Next.js.</p> <p>To create a React application, the official documentation recommends to start with a meta-framework. Alternatively, you can use the tool Create React App, optionally with TypeScript.</p>"},{"location":"language_guides/javascript/#angular","title":"Angular","text":"<p>Angular is a application framework by Google written in TypeScript. It is a full-blown framework, with many features included. It is therefore more used in enterprises and probably overkill for your average scientific project. Read more about what Angular is in the documentation.</p> <p>To create a Angular application see the installation docs.</p> <p>Angular also has a meta-framework called Analog.</p>"},{"location":"language_guides/javascript/#vuejs","title":"Vue.js","text":"<p>Vue.js is an open-source JavaScript framework for building user interfaces. Read about the use cases for Vue and reasons to use it in their introduction.</p> <p>To create a Vue application, read the quick start. It also has info on using TypeScript with Vue.</p> <p>A meta-framework for Vue is Nuxt.</p>"},{"location":"language_guides/javascript/#svelte","title":"Svelte","text":"<p>Svelte is a UI framework, that differs with most other frameworks in that is uses a compiler before shipping JavaScript to the client. Svelte applications are written in HTML, CSS and JS. Read more about Svelte in their overview.</p> <p>In their documentation, they recommend to use their meta-framework SvelteKit to create a Svelte application. It also supports TypeScript.</p>"},{"location":"language_guides/javascript/#solidjs","title":"Solid.js","text":"<p>A UI framework that focuses on performance and being developer friendly. Like React, it uses JSX. Read more about Solid here.</p> <p>To create a Solid application, check out the quick start. They also support TypeScript.</p> <p>Solid has a meta-framework called SolidStart.</p>"},{"location":"language_guides/javascript/#javascript-outside-of-the-browser","title":"JavaScript outside of the browser","text":"<p>Most JavaScript is run in web browsers, but if you want to run it outside of a browser (e.g. as a server or to run a script locally), you'll need a JavaScript runtime. These are the main runtimes available:</p> <ul> <li>Node.js is the most used runtime, mainly for being the only available runtime for a long time. This gives the advantage that there is a lot of documentation available (official and unofficial, e.g. forums) and that many tools are available for Node.js. It comes with a package manager (npm) that allows you to install packages from a huge library. Its installation instructions can be found here.</li> <li>Deno can be seen as a successor to Node.js and tries to improve on it in a few ways, most notably:</li> <li>built-in support for TypeScript</li> <li>a better security model</li> <li>built-in tooling, like a linter and formatter</li> <li>compiling to standalone executables</li> </ul> <p>Its installation instructions can be found here * Bun, the youngest runtime of the three. Its focus is on speed, reduced complexity and enhanced developer productivity (read more here). Just like Deno, it comes with built-in TypeScript support, can compile to standalone executables and it aims to be fully compatible with Node.js. Its installation instructions can be found here.</p> <p>A more comprehensive comparison can be found in this guide.</p>"},{"location":"language_guides/javascript/#which-runtime-to-choose","title":"Which runtime to choose?","text":"<p>To answer this question, you should consider what is important for you and your project.</p> <p>Choose Node.js if:</p> <ul> <li>you need a stable, mature and a well established runtime with a large community around it;</li> <li>you need to use dependencies that should most likely \"just work\";</li> <li>you cannot convince the people you work with to install something else;</li> <li>you don't need any particular feature of any of its competitors.</li> </ul> <p>Choose Deno if:</p> <ul> <li>you want a relatively mature runtime with a lot of features built in;</li> <li>you want out-of-the-box TypeScript support;</li> <li>you like its security model;</li> <li>you want a complete package with a linter and formatter included;</li> <li>you don't mind spending some time if something does not work directly.</li> </ul> <p>Choose Bun if:</p> <ul> <li>you are willing to take a risk using a relatively new runtime;</li> <li>you want out-of-the-box TypeScript support;</li> <li>you want to use one of Bun's particular features;</li> <li>you need maximum performance (though you should benchmark for your use case first and consider using a different programming language).</li> </ul>"},{"location":"language_guides/javascript/#editors-and-ides","title":"Editors and IDEs","text":"<p>These are some good JavaScript editors:</p> <ul> <li>WebStorm by JetBrains. It is free (as in monetary cost) for non-commercial use; otherwise you have to buy a licence. Most of its features are also available in other IDEs of JetBrains, like IntelliJ IDEA ultimate, PyCharm professional and Rider. You can compare the products of JetBrains here. Note that the free version of WebStorm will collect data anonymously, without the option to disable it. WebStorm comes with a lot of functionality included, but also gives access to a Marketplace of plugins.</li> <li>Visual Studio Code, an open source and free (as in monetary cost) editor by Microsoft. By default, it collects telemetry data, but that can be disabled. VSCode has a limited feature set out of the box, which can be enhanced with extensions.</li> </ul>"},{"location":"language_guides/javascript/#debugging","title":"Debugging","text":"<p>In web development, debugging is typically done in the browser. Read this article from W3Schools for more info.</p> <p>There is documentation for each browser on their dev tools:</p> <ul> <li>Firefox</li> <li>Chrome</li> <li>Edge</li> <li>Safari</li> </ul> <p>There are also debugging guides for the various JS runtimes: </p> <ul> <li>Node.js</li> <li>Deno</li> <li>Bun</li> </ul> <p>When using a (meta-)framework, also have a look at its documentation.</p> <p>Sometimes, the JavaScript code in the browser is not an exact copy of the code you see in your development environment, for example because the original source code is minified/uglified or transpiled before it's loaded in the browser. All major browsers can now deal with this through so-called source maps, which instruct the browser which symbol/line in a javascript file corresponds to which line in the human-readable source code. Look for the 'create sourcemaps' option when using minification/uglification/transpiling tools.</p>"},{"location":"language_guides/javascript/#hosting-data-files","title":"Hosting data files","text":"<p>To display web pages (HTML files) with JavaScript, you can't use any file system URL due to safety restrictions. You should use a web server (which may still serve files that are local). A simple web server can be started from the directory you want to host files with:</p> <pre><code>python3 -m http.server 8000\n</code></pre> <p>Then open the web browser to http://localhost:8000.</p>"},{"location":"language_guides/javascript/#documentation-idjs-docs","title":"Documentation :id=js-docs","text":"<p>JSDoc (similar to JavaDoc), parses your JavaScript files and automatically generates HTML documentation, based on the JSDoc comments you put in the code.</p>"},{"location":"language_guides/javascript/#testing","title":"Testing","text":"<p>The various runtimes have testing functionality included, so you don't have to install extra dependencies:</p> <ul> <li>Node.js</li> <li>Deno</li> <li>Bun</li> </ul> <p>If these don't suffice, a nice overview of popular testing frameworks can be found here.</p>"},{"location":"language_guides/javascript/#testing-with-browsers","title":"Testing with browsers","text":"<p>To interact with web browsers use Selenium.</p>"},{"location":"language_guides/javascript/#coding-style","title":"Coding style","text":""},{"location":"language_guides/javascript/#formatters","title":"Formatters","text":"<p>A formatter is a tool to make your source code look consistent and easy to look at. In web development, the most used formatter is Prettier, which can integrate with many editors. You could set up a GitHub action that rejects pull requests that are not formatted properly.</p> <p>When using Deno, you can also use its built-in formatter.</p> <p>An alternative to Prettier is Biome, which also includes a linter.</p> <p>In any case, remember to use tabs for indentation for the purpose of accessibility.</p>"},{"location":"language_guides/javascript/#linters","title":"Linters","text":"<p>A linter is a tool to check your code quality, in order to prevent bugs. The most used linter is ESLint. It has many integrations</p> <p>When using Deno, you can also use its built-in linter.</p> <p>An alternative to ESLint is Biome, which also includes a formatter.</p> <p>Also have a look at the Airbnb JavaScript Style Guide or the W3Schools page on JavaScript best practices.</p>"},{"location":"language_guides/javascript/#code-quality-analysis-tools-and-services","title":"Code quality analysis tools and services","text":"<p>For more in-depth analyses, you can use a code quality and analysis tool.</p> <ul> <li>SonarCloud is an open platform to manage code quality which can also show code coverage and count test results over time. It easily integrates with GitHub.</li> <li>Codacy can analyze many different languages using open source tools. It also offers GitHub integration.</li> <li>Code climate can analyze JavaScript (and Ruby, PHP). Can analyze Java (best supported), C, C++, Python, JavaScript and TypeScript.</li> </ul>"},{"location":"language_guides/javascript/#showing-code-examples","title":"Showing code examples","text":"<p>You can use jsfiddle, which shows you a live preview of your web page while you fiddle with the underlying HTML, JavaScript and CSS code.</p>"},{"location":"language_guides/javascript/#typescript","title":"TypeScript","text":"<p>https://www.typescriptlang.org/</p> <p>TypeScript is a typed superset of JavaScript which compiles to plain JavaScript. TypeScript adds static typing to JavaScript, which makes it easier to scale up in people and lines of code.</p> <p>At the Netherlands eScience Center we prefer TypeScript to JavaScript as it will lead to more sustainable software.</p> <p>This section highlights the differences with JavaScript. For topics without significant differences, like IDEs, code style etc., see the respective JavaScript section.</p>"},{"location":"language_guides/javascript/#getting-started_1","title":"Getting Started","text":"<p>To learn about TypeScript, the following resources are available:</p> <ul> <li>Official TypeScript documentation and tutorial</li> <li>Single video tutorial and playlist tutorial</li> <li>Tutorials on debugging TypeScript in Chrome and Firefox. If you are using a framework, consult the documentation of that framework for additional ways of debugging</li> <li>The Definitive TypeScript 5.0 Guide</li> <li>The W3Schools TypeScript tutorial</li> </ul>"},{"location":"language_guides/javascript/#quickstart","title":"Quickstart","text":"<p>To install TypeScript compiler run, check out the official documentation. Note that Deno and Bun support TypeScript out of the box.</p>"},{"location":"language_guides/javascript/#dealing-with-types","title":"Dealing with Types","text":"<p>In TypeScript, variables are typed and these types are checked. This implies that when using libraries, the types of these libraries need to be installed. More and more libraries ship with type declarations in them so they can be used directly. These libraries will have a \"typings\" key in their <code>package.json</code>. When a library does not ship with type declarations then the libraries <code>@types/&lt;library-name&gt;</code> package must be installed using npm:</p> <pre><code>npm install --save-dev @types/&lt;library-name&gt;\n</code></pre> <p>For example say we want to use the <code>react</code> package which we installed using <code>npm</code>: <pre><code>npm install react --save\n</code></pre></p> <p>To be able to use its functionality in TypeScript we need to install the typings.</p> <p>Install it with:</p> <pre><code>npm install --save-dev @types/react\n</code></pre> <p>The <code>--save-dev</code> flag saves this installation to the package.json file as a development dependency. Do not use <code>--save</code> for types because a production build will have been transpiled to JavaScript and has no use for TypeScript types.</p>"},{"location":"language_guides/javascript/#debugging_1","title":"Debugging","text":"<p>In web development, debugging is typically done in the browser. TypeScript cannot be run directly in the web browser, so it must be transpiled to JavaScript. To map a breakpoint in the browser to a line in the original TypeScript file source maps are required. Most frameworks have a project build system which generate source maps. For more info, see the Javascript section on debugging</p>"},{"location":"language_guides/javascript/#documentation","title":"Documentation","text":"<p>Just like JSDoc for JavaScript, TypeDoc can automatically generate HTML documentation for your code.</p>"},{"location":"language_guides/languages_overview/","title":"Languages overview","text":"<p>Page maintainer: Patrick Bos @egpbos</p> <p>This chapter provides practical info on each of the main programming languages of the Netherlands eScience Center.</p> <p>This info is (on purpose) high level, try to provide \"default\" options, and mostly link to more info.</p> <p>Each chapter should contain:</p> <ul> <li>Intro: philosophy, typical usecases.</li> <li>Recommended sources of information</li> <li>Installing compilers and runtimes</li> <li>Editors and IDEs</li> <li>Coding style conventions</li> <li>Building and packaging code</li> <li>Testing</li> <li>Code quality analysis tools and services</li> <li>Debugging and Profiling</li> <li>Logging</li> <li>Writing documentation</li> <li>Recommended additional packages and libraries</li> <li>Available templates</li> </ul>"},{"location":"language_guides/languages_overview/#preferred-languages","title":"Preferred Languages","text":"<p>At the Netherlands eScience Center we prefer Java and Python over C++ and Perl, as these languages in general produce more sustainable code. It is not always possible to choose which libraries we use, as almost all projects have existing code as a starting point.</p> <p>(In alphabetical order)</p> <ul> <li>Java</li> <li>JavaScript (preferably Typescript)</li> <li>Python</li> <li>OpenCL and CUDA</li> <li>R</li> </ul>"},{"location":"language_guides/languages_overview/#selecting-tools-and-libraries","title":"Selecting tools and libraries","text":"<p>On GitHub there is a concept of an \"awesome list\", that collects awesome libraries and tools on some topic. For instance, here is one for Python: https://github.com/vinta/awesome-python</p> <p>Now, someone has been smart enough to see the pattern, and has created an awesome list of awesome lists: https://awesome.re/</p> <p>Highly recommented to get some inspiration on available tools and libraries!</p>"},{"location":"language_guides/languages_overview/#development-services","title":"Development Services","text":"<p>To do development in any language you first need infrastructure (code hosting, ci, etc). Luckily a lot is available for free now.</p> <p>See this list: https://github.com/ripienaar/free-for-dev</p>"},{"location":"language_guides/python/","title":"Python","text":"<p>Page maintainer: Bouwe Andela @bouweandela</p> <p>Python is the \"dynamic language of choice\" of the Netherlands eScience Center. We use it for data analysis and data science projects using the SciPy stack and Jupyter notebooks, and for many other types of projects: workflow management, visualization, NLP, web-based tools and much more. It is a good default choice for many kinds of projects due to its generic nature, its large and broad ecosystem of third-party modules and its compact syntax which allows for rapid prototyping. It is not the language of maximum performance, although in many cases performance critical components can be easily replaced by modules written in faster, compiled languages like C(++) or Cython.</p> <p>The philosophy of Python is summarized in the Zen of Python. In Python, this text can be retrieved with the <code>import this</code> command.</p>"},{"location":"language_guides/python/#project-setup","title":"Project setup","text":"<p>When starting a new Python project, consider using our Python template. This template provides a basic project structure, so you can spend less time setting up and configuring your new Python packages, and comply with the software guide right from the start.</p>"},{"location":"language_guides/python/#use-python-3-avoid-2","title":"Use Python 3, avoid 2","text":"<p>Python 2 and Python 3 have co-existed for a long time, but starting from 2020, development of Python 2 is officially abandoned, meaning Python 2 will no longer be improved, even in case of security issues. If you are creating a new package, use Python 3. It is possible to write Python that is both Python 2 and Python 3 compatible (e.g. using Six), but only do this when you are 100% sure that your package won't be used otherwise. If you need Python 2 because of old, incompatible Python 2 libraries, strongly consider upgrading those libraries to Python 3 or replacing them altogether. Building and/or using Python 2 is probably discouraged even more than, say, using Fortran 77, since at least Fortran 77 compilers are still being maintained.</p> <ul> <li>Things you\u2019re probably not using in Python 3 \u2013 but should</li> <li>Six: Python 2 and 3 Compatibility Library</li> <li>2to3: Automated Python 2 to 3 code translation</li> <li>python-modernize: wrapper around 2to3</li> </ul>"},{"location":"language_guides/python/#learning-python","title":"Learning Python","text":"<ul> <li>A popular way to learn Python is by doing it the hard way at http://learnpythonthehardway.org/</li> <li>Using <code>pylint</code> and <code>yapf</code> while learning Python is an easy way to get familiar with best practices and commonly used coding styles</li> </ul>"},{"location":"language_guides/python/#dependencies-and-package-management","title":"Dependencies and package management","text":"<p>To install Python packages use <code>pip</code> or <code>conda</code> (or both, see also what is the difference between pip and conda?).</p> <p>If you are planning on distributing your code at a later stage, be aware that your choice of package management may affect your packaging process. See Building and packaging for more info.</p>"},{"location":"language_guides/python/#use-virtual-environments","title":"Use virtual environments","text":"<p>We strongly recommend creating isolated \"virtual environments\" for each Python project. These can be created with <code>venv</code> or with <code>conda</code>. Advantages over installing packages system-wide or in a single user folder:</p> <ul> <li>Installs Python modules when you are not root.</li> <li>Contains all Python dependencies so the environment keeps working after an upgrade.</li> <li>Keeps environments clean for each project, so you don't get more than you need (and can easily reproduce that minimal working situation).</li> <li>Lets you select the Python version per environment, so you can test code compatibility between Python versions</li> </ul>"},{"location":"language_guides/python/#pip-a-virtual-environment","title":"Pip + a virtual environment","text":"<p>If you don't want to use <code>conda</code>, create isolated Python environments with the standard library <code>venv</code> module. If you are still using Python 2, <code>virtualenv</code> and <code>virtualenvwrapper</code> can be used instead.</p> <p>With <code>venv</code> and <code>virtualenv</code>, <code>pip</code> is used to install all dependencies. An increasing number of packages are using <code>wheel</code>, so <code>pip</code> downloads and installs them as binaries. This means they have no build dependencies and are much faster to install.</p> <p>If the installation of a package fails because of its non-Python extensions or system library dependencies and you are not root, you could switch to <code>conda</code> (see below).</p>"},{"location":"language_guides/python/#conda","title":"Conda","text":"<p>Conda can be used instead of venv and pip, since it is both an environment manager and a package manager. It easily installs binary dependencies, like Python itself or system libraries. Installation of packages that are not using <code>wheel</code>, but have a lot of non-Python code, is much faster with Conda than with <code>pip</code> because Conda does not compile the package, it only downloads compiled packages. The disadvantage of Conda is that the package needs to have a Conda build recipe. Many Conda build recipes already exist, but they are less common than the <code>setuptools</code> configuration that generally all Python packages have.</p> <p>There are two main \"official\" distributions of Conda: Anaconda and Miniconda (and variants of the latter like miniforge, explained below). Anaconda is large and contains a lot of common packages, like numpy and matplotlib, whereas Miniconda is very lightweight and only contains Python. If you need more, the <code>conda</code> command acts as a package manager for Python packages. If installation with the <code>conda</code> command is too slow for your purposes, it is recommended that you use <code>mamba</code> instead.</p> <p>For environments where you do not have admin rights (e.g. DAS-6) either Anaconda or Miniconda is highly recommended since the installation is very straightforward. The installation of packages through Conda is very robust.</p> <p>A possible downside of Anaconda is the fact that this is offered by a commercial supplier, but we don't foresee any vendor lock-in issues, because all packages are open source and can still be obtained elsewhere. Do note that since 2020, Anaconda has started to ask money from large institutes for downloading packages from their main channel (called the <code>default</code> channel) through <code>conda</code>. This does not apply to universities and most research institutes, but could apply to some government institutes that also perform research and definitely applies to large for-profit companies. Be aware of this when choosing the distribution channel for your package. An alternative, community-driven Conda distribution that avoids this problem altogether because it only installs packages from <code>conda-forge</code> by default is miniforge. Miniforge includes both the faster <code>mamba</code> as well as the traditional <code>conda</code>.</p>"},{"location":"language_guides/python/#building-and-packaging-code","title":"Building and packaging code","text":""},{"location":"language_guides/python/#making-an-installable-package","title":"Making an installable package","text":"<p>To create an installable Python package you will have to create a <code>pyproject.toml</code> file. This will contain three kinds of information: metadata about your project, information on how to build and install your package, and configuration settings for any tools your project may use. Our Python template already does this for you.</p>"},{"location":"language_guides/python/#project-metadata","title":"Project metadata","text":"<p>Your project metadata will be under the <code>[project]</code> header, and includes such information as the name, version number, description and dependencies. The Python Packaging User Guide has more information on what else can or should be added here. For your dependencies, you should keep version constraints to a minimum; use, in order of descending preference: no constraints, lower bounds, lower + upper bounds, exact versions. Use of <code>requirements.txt</code> is discouraged, unless necessary for something specific, see the discussion here.</p> <p>It is best to keep track of direct dependencies for your project from the start and list these in your <code>pyproject.toml</code> If instead you are writing a new <code>pyproject.toml</code> for an existing project, a recommended way to find all direct dependencies is by running your code in a clean environment (probably by running your test suite) and installing one by one the dependencies that are missing, as reported by the ensuing errors. It is possible to find the full list of currently installed packages with <code>pip freeze</code> or <code>conda list</code>, but note that this is not ideal for listing dependencies in <code>pyproject.toml</code>, because it also lists all dependencies of the dependencies that you use.</p>"},{"location":"language_guides/python/#build-system","title":"Build system","text":"<p>Besides specifying your project's own metadata, you also have to specify a build-system under the <code>[build-system]</code> header. We currently recommend using <code>hatchling</code> or <code>setuptools</code>. Note that Python's build system landscape is still in flux, so be sure to look upthe some current practices in the packaging guide's section on build backends and authoritative blogs like this one. One important thing to note is that use of <code>setup.py</code> and <code>setup.cfg</code> has been officially deprecated and we should migrate away from that.</p>"},{"location":"language_guides/python/#tool-configuration","title":"Tool configuration","text":"<p>Finally, <code>pyproject.toml</code> can be used to specify the configuration for any other tools like <code>pytest</code>, <code>ruff</code> and <code>mypy</code> your project may use. Each of these gets their own section in your <code>pyproject.toml</code> instead of using their own file, saving you from having dozens of such files in your project.</p>"},{"location":"language_guides/python/#installation","title":"Installation","text":"<p>When the <code>pyproject.toml</code> is written, your package can be installed with <pre><code>pip install -e .\n</code></pre> The <code>-e</code> flag will install your package in editable mode, i.e. it will create a symlink to your package in the installation location  instead of copying the package. This is convenient when developing, because any changes you make to the source code will immediately be available for use in the installed version.</p> <p>Set up continuous integration to test your installation setup. You can use <code>pyroma</code> as a linter for your installation configuration.</p>"},{"location":"language_guides/python/#packaging-and-distributing-your-package","title":"Packaging and distributing your package","text":"<p>For packaging your code, you can either use <code>pip</code> or <code>conda</code>. Neither of them is better than the other -- they are different; use the one which is more suitable for your project. <code>pip</code> may be more suitable for distributing pure python packages, and it provides some support for binary dependencies using <code>wheels</code>. <code>conda</code> may be more suitable when you have external dependencies which cannot be packaged in a wheel.</p>"},{"location":"language_guides/python/#build-via-the-python-package-index-pypi-so-that-the-package-can-be-installed-with-pip","title":"Build via the Python Package Index (PyPI) so that the package can be installed with pip","text":"<ul> <li>General instructions </li> <li>We recommend to configure GitHub Actions to upload the package to PyPI automatically for each release. <ul> <li>For new repositories, it is recommended to use trusted publishing because it is more secure than using secret tokens from GitHub.<ul> <li>For a workflow using secret tokens instead, see this example workflow in DIANNA.</li> </ul> </li> <li>You can follow these instructions to set up GitHub Actions workflows with trusted publishing. <ul> <li>The <code>verbose</code> option for pypi workflows is useful to see why a workflow failed.</li> <li>To avoid unnecessary workflow runs, you can follow the example in the sirup package: manually trigger pushes to pypi and investigate potential bugs during this process with a manual upload.</li> </ul> </li> </ul> </li> <li>Manual uploads with twine <ul> <li>Because PyPI and Test PyPI require Two-Factor Authentication per January 2024, you need to mimick GitHub's trusted publishing to publish manually with <code>twine</code>.</li> <li>You can follow the section on \"The manual way\" as described here.</li> </ul> </li> <li>Additional guidelines:<ul> <li>Packages should be uploaded to PyPI using your own account</li> <li>For packages developed in a team or organization, it is recommended that you create a team or organizational account on PyPI and add that as a collaborator with the owner rule. This will allow your team or organization to maintain the package even if individual contributors at some point move on to do other things. At the Netherlands eScience Center, we are a fairly small organization, so we use a single backup account (<code>nlesc</code>).</li> <li>When distributing code through PyPI, non-python files (such as <code>requirements.txt</code>) will not be packaged automatically, you need to add them to a <code>MANIFEST.in</code> file.</li> <li>To test whether your distribution will work correctly before uploading to PyPI, you can run <code>python -m build</code> in the root of your repository. Then try installing your package with <code>pip install dist/&lt;your_package&gt;tar.gz.</code></li> <li><code>python -m build</code> will also build Python wheels, the current standard for distributing Python packages. This will work out of the box for pure Python code, without C extensions. If C extensions are used, each OS needs to have its own wheel. The manylinux Docker images can be used for building wheels compatible with multiple Linux distributions. Wheel building can be automated using GitHub Actions or another CI solution, where you can build on all three major platforms using a build matrix.</li> </ul> </li> </ul>"},{"location":"language_guides/python/#build-using-conda","title":"Build using conda","text":"<ul> <li>Make use of conda-forge whenever possible, since it provides many automated build services that save you tons of work, compared to using your own conda repository. It also has a very active community for when you need help.</li> <li>Use BioConda or custom channels (hosted on GitHub) as alternatives if need be.</li> </ul>"},{"location":"language_guides/python/#editors-and-ides","title":"Editors and IDEs","text":"<p>Every major text editor supports Python, either natively or through plugins. At the Netherlands eScience Center, some popular editors or IDEs are:</p> <ul> <li>vscode holds the middle ground between a lightweight text editor and a full-fledged language-dedicated IDE.</li> <li>vim or <code>emacs</code> (don't forget to install plugins to get the most out of these two), two versatile classic powertools that can also be used through remote SSH connection when needed.</li> <li>JetBrains PyCharm is the Python-specific IDE of choice. PyCharm Community Edition is free and open source; the source code is available in the python folder of the IntelliJ repository.</li> </ul>"},{"location":"language_guides/python/#coding-style-conventions","title":"Coding style conventions","text":"<p>The style guide for Python code is PEP8 and for docstrings it is PEP257. We highly recommend following these conventions, as they are widely agreed upon to improve readability. To make following them significantly easier, we recommend using a linter.</p> <p>Many linters exists for Python. The most popular one is currently Ruff. Although it is new (see the website for the complete function parity comparison with alternatives), it works well and has an active community. An alternative is <code>prospector</code>, a tool for running a suite of linters, including, among others pycodestyle, pydocstyle, pyflakes, pylint, mccabe and pyroma. Some of these tools have seen decreasing community support recently, but it is still a good alternative, having been a defining community default for years.</p> <p>Most of the above tools can be integrated in text editors and IDEs for convenience.</p> <p>Autoformatting tools like <code>yapf</code> and <code>black</code> can automatically format code for optimal readability. <code>yapf</code> is configurable to suit your (team's) preferences, whereas <code>black</code> enforces the style chosen by the <code>black</code> authors. The <code>isort</code> package automatically formats and groups all imports in a standard, readable way.</p> <p>Ruff can do autoformatting as well and can function as a drop-in replacement of <code>black</code> and <code>isort</code>.</p>"},{"location":"language_guides/python/#testing","title":"Testing","text":"<p>Use pytest as the basis for your testing setup. This is preferred over the <code>unittest</code> standard library, because it has a much more concise syntax and supports many useful features.</p> <p>It has many plugins. For linting, we have found <code>pytest-pycodestyle</code>, <code>pytest-pydocstyle</code>, <code>pytest-mypy</code> and <code>pytest-flake8</code> to be useful. Other plugins we had good experience with are <code>pytest-cov</code>, <code>pytest-html</code>, <code>pytest-xdist</code> and <code>pytest-nbmake</code>.</p> <p>Creating mocks can also be done within the pytest framework by using the <code>mocker</code> fixture provided by the <code>pytest-mock</code> plugin or by using <code>MagicMock</code> and <code>patch</code> from <code>unittest</code>. For a general explanation about mocking, see the standard library docs on mocking.</p> <p>To run your test suite, it can be convenient to use <code>tox</code>. Testing with <code>tox</code> allows for keeping the testing environment separate from your development environment. The development environment will typically accumulate (old) packages during development that interfere with testing; this problem is avoided by testing with <code>tox</code>.</p>"},{"location":"language_guides/python/#code-coverage","title":"Code coverage","text":"<p>When you have tests it is also a good to see which source code is exercised by the test suite. Code coverage can be measured with the coverage Python package. The coverage package can also generate html reports which show which line was covered. Most test runners have have the coverage package integrated.</p> <p>The code coverage reports can be published online using a code quality service or code coverage services. Preferred is to use one of the code quality service which also handles code coverage listed below. If this is not possible or does not fit then use a generic code coverage service such as Codecov or Coveralls.</p>"},{"location":"language_guides/python/#code-quality-analysis-tools-and-services","title":"Code quality analysis tools and services","text":"<p>Code quality service is explained in the The Turing Way. There are multiple code quality services available for Python, all of which have their pros and cons. See The Turing Way for links to lists of possible services. We currently setup Sonarcloud by default in our Python template. To reproduce the Sonarcloud pipeline locally, you can use SonarLint in your IDE. If you use another editor, perhaps it is more convenient to pick another service like Codacy or Codecov.</p>"},{"location":"language_guides/python/#debugging-and-profiling","title":"Debugging and profiling","text":""},{"location":"language_guides/python/#debugging","title":"Debugging","text":"<ul> <li>Python has its own debugger called pdb. It is a part of the Python distribution.</li> <li>pudb is a console-based Python debugger which can easily be installed using pip.</li> <li>If you are looking for IDEs with debugging capabilities, see the Editors and IDEs section.</li> <li>If you are using Windows, Python Tools for Visual Studio adds Python support for Visual Studio.</li> <li> <p>If you would like to integrate pdb with <code>vim</code>, you can use Pyclewn.</p> </li> <li> <p>List of other available software can be found on the Python wiki page on debugging tools.</p> </li> <li> <p>If you are looking for some tutorials to get started:</p> <ul> <li>https://pymotw.com/2/pdb</li> <li>https://github.com/spiside/pdb-tutorial</li> <li>https://www.jetbrains.com/help/pycharm/2016.3/debugging.html</li> <li>https://waterprogramming.wordpress.com/2015/09/10/debugging-in-python-using-pycharm/</li> <li>http://www.pydev.org/manual_101_run.html</li> </ul> </li> </ul>"},{"location":"language_guides/python/#profiling","title":"Profiling","text":"<p>There are a number of available profiling tools that are suitable for different situations.</p> <ul> <li>cProfile measures number of function calls and how much CPU time they take. The output can be further analyzed using the <code>pstats</code> module.</li> <li>For more fine-grained, line-by-line CPU time profiling, two modules can be used:<ul> <li>line_profiler provides a function decorator that measures the time spent on each line inside the function.</li> <li>pprofile is less intrusive; it simply times entire Python scripts line-by-line. It can give output in callgrind format, which allows you to study the statistics and call tree in <code>kcachegrind</code> (often used for analyzing c(++) profiles from <code>valgrind</code>).</li> </ul> </li> </ul> <p>More realistic profiling information can usually be obtained by using statistical or sampling profilers. The profilers listed below all create nice flame graphs.</p> <ul> <li>vprof</li> <li>Pyflame</li> <li>nylas-perftools</li> </ul>"},{"location":"language_guides/python/#logging","title":"Logging","text":"<ul> <li>logging module is the most commonly used tool to track events in Python code.</li> <li>Tutorials:<ul> <li>Official Python Logging Tutorial</li> <li>http://docs.python-guide.org/en/latest/writing/logging</li> <li>Python logging best practices</li> </ul> </li> </ul>"},{"location":"language_guides/python/#writing-documentation","title":"Writing Documentation","text":"<p>Python uses Docstrings for function level documentation. You can read a detailed description of docstring usage in PEP 257. The default location to put HTML documentation is Read the Docs. You can connect your account at Read the Docs to your GitHub account and let the HTML be generated automatically using Sphinx.</p>"},{"location":"language_guides/python/#autogenerating-the-documentation","title":"Autogenerating the documentation","text":"<p>There are several tools that automatically generate documentation from docstrings. At the eScience Center, we mostly use Sphinx, which uses reStructuredText as its markup language, but can be extended to use Markdown as well.</p> <ul> <li>Sphinx quickstart</li> <li>reStructuredText Primer</li> <li>Instead of using reST, Sphinx can also generate documentation from the more readable NumPy style or Google style docstrings. The Napoleon extension needs to be enabled.</li> </ul> <p>We recommend using the Google documentation style. Use <code>sphinx-build</code> to build your documentation.</p> <p>You can also integrate entire Jupyter notebooks into your HTML Sphinx output with nbsphinx. This way, your demo notebooks, for instance, can double as documentation. Of course, the notebooks will not be interactive in the compiled HTMl, but they will include all code and output cells.</p>"},{"location":"language_guides/python/#recommended-additional-packages-and-libraries","title":"Recommended additional packages and libraries","text":""},{"location":"language_guides/python/#general-scientific","title":"General scientific","text":"<ul> <li>NumPy</li> <li>SciPy</li> <li>Pandas data analysis toolkit</li> <li>scikit-learn: machine learning in Python</li> <li>Cython speed up Python code by using C types and calling C functions</li> <li>dask larger than memory arrays and parallel execution</li> </ul>"},{"location":"language_guides/python/#ipython-and-jupyter-notebooks-aka-ipython-notebooks","title":"IPython and Jupyter notebooks (aka IPython notebooks)","text":"<p>IPython is an interactive Python interpreter -- very much the same as the standard Python interactive interpreter, but with some extra features (tab completion, shell commands, in-line help, etc).</p> <p>Jupyter notebooks (formerly know as IPython notebooks) are browser based interactive Python enviroments. It incorporates the same features as the IPython console, plus some extras like in-line plotting.  Look at some examples to find out more. Within a notebook you can alternate code with Markdown comments (and even LaTeX), which is great for reproducible research. Notebook extensions adds extra functionalities to notebooks. JupyterLab is a web-based environment with a lot of improvements and integrated tools.</p> <p>Jupyter notebooks contain data that makes it hard to nicely keep track of code changes using version control. If you are using git, you can add filters that automatically remove output cells and unneeded metadata from your notebooks. If you do choose to keep output cells in the notebooks (which can be useful to showcase your code's capabilities statically from GitHub) use ReviewNB to automatically create nice visual diffs in your GitHub pull request threads. It is good practice to restart the kernel and run the notebook from start to finish in one go before saving and committing, so you are sure that everything works as expected.</p>"},{"location":"language_guides/python/#visualization","title":"Visualization","text":"<ul> <li>Matplotlib has been the standard in scientific visualization. It supports quick-and-dirty plotting through the <code>pyplot</code> submodule. Its object oriented interface can be somewhat arcane, but is highly customizable and runs natively on many platforms, making it compatible with all major OSes and environments. It supports most sources of data, including native Python objects, Numpy and Pandas.<ul> <li>Seaborn is a Python visualisation library based on Matplotlib and aimed towards statistical analysis. It supports numpy, pandas, scipy and statmodels.</li> </ul> </li> <li>Web-based:<ul> <li>Bokeh is Interactive Web Plotting for Python.</li> <li>Plotly is another platform for interactive plotting through a web browser, including in Jupyter notebooks.</li> <li>altair is a grammar of graphics style declarative statistical visualization library. It does not render visualizations itself, but rather outputs Vega-Lite JSON data. This can lead to a simplified workflow.</li> <li>ggplot is a plotting library imported from R.</li> </ul> </li> </ul>"},{"location":"language_guides/python/#parallelisation","title":"Parallelisation","text":"<p>CPython (the official and mainstream Python implementation) is not built for parallel processing due to the global interpreter lock. Note that the GIL only applies to actual Python code, so compiled modules like e.g. <code>numpy</code> do not suffer from it.</p> <p>Having said that, there are many ways to run Python code in parallel: * The multiprocessing module is the standard way to do parallel executions in one or multiple machines, it circumvents the GIL by creating multiple Python processess. * A much simpler alternative in Python 3 is the <code>concurrent.futures</code> module. * IPython / Jupyter notebooks have built-in parallel and distributed computing capabilities * Many modules have parallel capabilities or can be compiled to have them. * At the eScience Center, we have developed the Noodles package for creating computational workflows and automatically parallelizing it by dispatching independent subtasks to parallel and/or distributed systems.</p>"},{"location":"language_guides/python/#web-frameworks","title":"Web Frameworks","text":"<p>There are convenient Python web frameworks available:</p> <ul> <li>flask</li> <li>CherryPy</li> <li>Django</li> <li>bottle (similar to flask, but a bit more light-weight for a JSON-REST service)</li> </ul> <p>We recommend <code>flask</code>.</p>"},{"location":"language_guides/python/#nlptext-mining","title":"NLP/text mining","text":"<ul> <li>nltk Natural Language Toolkit</li> <li>Pattern: web/text mining module</li> <li>gensim: Topic modeling</li> </ul>"},{"location":"language_guides/python/#creating-programs-with-command-line-arguments","title":"Creating programs with command line arguments","text":"<ul> <li>For run-time configuration via command-line options, the built-in <code>argparse</code> module usually suffices.</li> <li>A more complete solution is <code>ConfigArgParse</code>. This (almost) drop-in replacement for <code>argparse</code> allows you to not only specify configuration options via command-line options, but also via (ini or yaml) configuration files and via environment variables.</li> <li>Other popular libraries are <code>click</code> and <code>fire</code>.</li> </ul>"},{"location":"language_guides/r/","title":"What is R?","text":"<p>Page maintainer: Malte L\u00fcken @maltelueken</p> <p>R is a functional programming language and software environment for statistical computing and graphics: https://www.r-project.org/.</p>"},{"location":"language_guides/r/#philosophy-and-typical-use-cases","title":"Philosophy and typical use cases","text":"<p>R is particularly popular in the social, health, and biological sciences where it is used for statistical modeling. R can also be used for signal processing (e.g. FFT), machine learning, image analyses, and natural language processing. The R syntax is similar in compactness and readability as python and matlab by which it serves as a good prototyping environment in science.</p> <p>One of the strengths of R is the large number of available open source statistical packages, often developed by domain experts. For example, R-package Seewave is specialised in sound analyses. Packages are typically released on CRAN The Comprehensive R Archive Network.</p> <p>A few remarks for readers familiar with Python: * Compared with Python, R does not need a notebook to program interactively. In RStudio, an IDE that is installed separately, the user can run sections of the code by selecting them and pressing Ctrl+Enter. Consequently the user can quickly transition from working with scripts to working interactively using the Ctrl+Enter. * Numbering in R starts with 1 and not with 0.</p>"},{"location":"language_guides/r/#recommended-sources-of-information","title":"Recommended sources of information","text":"<p>Some R packages have their own google.group. All R functions come with documentation in a standardized format. To learn R see the following resources: * R for Data Science by Hadley Wickham, * Advanced R by Hadley Wickham, * Writing better R code by Laurent Gatto.</p> <p>Further, stackoverflow and standard search engines can lead you to answers to issues.</p>"},{"location":"language_guides/r/#getting-started","title":"Getting started","text":""},{"location":"language_guides/r/#setting-up-r","title":"Setting up R","text":"<p>To install R check detailed description at CRAN website.</p>"},{"location":"language_guides/r/#ide","title":"IDE","text":"<p>R programs can be written in any text editor. R code can be run from the command line or interactively within R environment, that can be started with <code>R</code> command in the shell. To quit R environment type <code>q()</code>.</p> <p>RStudio is a free powerful integrated development environment (IDE) for R. It features editor with code completion, command line environment, file manager, package manager and history lookup among others. You will have to install RStudio in addition to installing R. Please note that updating RStudio does not automatically update R and the other way around.</p> <p>Within RStudio you can work on ad-hoc code or create a project. Compared with Python an R project is a bit like a virtual environment as it preserves the workspace and installed packages for that project. Creating a project is needed to build an R package. A project is created via the menu at the top of the screen.</p>"},{"location":"language_guides/r/#installing-compilers-and-runtimes","title":"Installing compilers and runtimes","text":"<p>Not needed as most functions in R are already compiled in C, nevertheless R has compiling functionality as described in the R manual. See overview by Hadley Wickham.</p>"},{"location":"language_guides/r/#coding-style-conventions","title":"Coding style conventions","text":"<p>It is good to follow the R style conventions as posted by Hadley Wickham, which is seems compatible with the R style convention as posted by Google.</p> <p>One point in both style conventions that has resulted in some discussion is the '&lt;-' syntax for variable assignment. In the majority of R tutorials and books you will see that authors use this syntax, e.g. 'a &lt;- 3' to assign value 3 to object 'a'. Please note that R syntax 'a = 3' will preform exactly the same operation in 99.9% of situations. The = syntax has less keystrokes and could therefore be considered more efficient and readable. Further, the = syntax avoids the risk for typos like a &lt; -1, which will produce a boolean if 'a' exists, and a &lt;- 1 which will produce an object 'a' with a numeric value. Further, the = syntax may be more natural for those who already use it in other computing languages.</p> <p>The difference between '&lt;-' and '=' is mainly related to scoping. See the official R definition for more information. The example below demonstrates the difference in behaviour:</p> <p>Define a simple function named addone to add 1 to the function input: - addone = function(x) return(x + 1) - addone(3)   - will produce 4 - addone(b=3)   - will throw an error message because the function does not know argument b - addone(b&lt;-3)   - will produce 4 as it will first assign 3 to b and then uses b as value for the first argument in addone, which happens to be x - addone(x=3)   - will produce 4 as it will assign 3 to known function argument x</p> <p>The &lt;- supporters will argue that this example demonstrates that = should be avoided. However, it also demonstrates that = syntax can work in the context of function input if = is only used for assigning values to input arguments that are expected by the function (x in the example above) and to never introduce new R objects as part of a function call (b in the example above).</p> <p>From a computer science perspective it is probably best to adhere to the &lt;- convention. From a domain science perspective it is understandable to use =. The code performs exactly the same and guarantees that new objects created as part of a function call result in an error. Please note that it is also possible to develop code with = syntax and to transfer it to &lt;- syntax once the code is finished, the formatR package offers tools for doing this. The CRAN repository for R packages accepts both forms of syntax.</p>"},{"location":"language_guides/r/#recommended-additional-packages-and-libraries","title":"Recommended additional packages and libraries","text":""},{"location":"language_guides/r/#plotting-with-basic-functions-and-ggplot2-and-ggvis","title":"Plotting with basic functions and ggplot2 and ggvis","text":"<p>For a generic impression of what R can do see: https://www.r-graph-gallery.com/all-graphs</p> <p>The basic R installation comes with a wide range of functions to plot data to a window on your screen or to a file. If you need to quickly inspect your data or create a custom-made static plot then the basic functions offer the building blocks to do the job. There is a Statmethods.net tutorial with some examples of plotting options in R.</p> <p>However, externally contributed plotting packages may offer easier syntax or convenient templates for creating plots. The most popular and powerful contributed graphics package is ggplot2. Interactive plots can be made with ggvis package and embeded in web application, and this tutorial.</p> <p>In summary, it is good to familiarize yourself with both the basic plotting functions as well as the contributed graphics packages. In theory, the basic plot functions can do everything that ggplot2 can do, it is mostly a matter of how much you like either syntax and how much freedom you need to tailor the visualisation to your use case.</p>"},{"location":"language_guides/r/#building-interactive-web-applications-with-shiny","title":"Building interactive web applications with shiny","text":"<p>Thanks to shiny.app it is possible to make interactive web application in R without the need to write javascript or html.</p>"},{"location":"language_guides/r/#building-reports-with-knitr","title":"Building reports with knitr","text":"<p>knitr is an R package designed to build dynamic reports in R. It's possible to generate on the fly new pdf or html documents with results of computations embedded inside.</p>"},{"location":"language_guides/r/#preparing-data-for-analysis","title":"Preparing data for analysis","text":"<p>There are packages that ease tidying up messy data, e.g. tidyr and reshape2. The idea of tidy and messy data is explained in a tidy data paper by Hadley Wickham. There is also the google group manipulatr to discuss topics related to data manipulation in R.</p>"},{"location":"language_guides/r/#speeding-up-code","title":"Speeding up code","text":"<p>As in many computing languages loops should be avoided in R. Here is a list of tricks to speed up your code:</p> <ul> <li>read.table() is sometimes faster than read.csv()</li> <li>ifelse()</li> <li>lapply()</li> <li>sapply()</li> <li>mapply()</li> <li>grep()</li> <li>%in% for testing whether and where values in one object occur in another object</li> <li>aggregate()</li> <li>which() for identifying which object indices match a certain condition</li> <li>table() for getting a frequency table of categorical data</li> <li>grep()</li> <li>gsub()</li> <li>dplyr package, see also</li> </ul> <p>Use ?functionname to access fucntion documentation.</p>"},{"location":"language_guides/r/#package-development","title":"Package development","text":""},{"location":"language_guides/r/#building-r-packages","title":"Building R packages","text":"<p>There is a great tutorial written by Hadley Wickam describing all the nitty gritty of building your own package in R. It's called R packages.</p>"},{"location":"language_guides/r/#package-documentation","title":"Package documentation","text":"<p>Read Documentation chapter of Hadleys R packages book for details about documenting R code.</p> <p>Customary R uses <code>.Rd</code> files in <code>/man</code> directory for documentation. These files and folders are automatically created by RStudio when you create a new project from your existing R-function files.</p> <p>If you use 'roxygen' function level comments starting with <code>#'</code> are recognised by <code>roxygen</code> and are used to automatically generate .Rd files. Read more about <code>roxygen</code> syntax on it's github page. <code>roxygen</code> will also populate <code>NAMESPACE</code> file which is necessary to manage package level imports.</p> <p>R function documentation offers plenty of space to document the functionality, including code examples, literature references, and links to related functions. Nevertheless, it can sometimes be helpful for the user to also have a more generic description of the package with for example use-cases. You can do this with a <code>vignette</code>. Read more about vignettes in Package documentation chapter of Hadleys R packages book.</p>"},{"location":"language_guides/r/#available-templates","title":"Available templates","text":"<ul> <li>https://rapporter.github.io/rapport/</li> <li>https://shiny.posit.co/r/articles/build/templates/</li> <li>https://bookdown.org/yihui/rmarkdown/document-templates.html</li> </ul>"},{"location":"language_guides/r/#testing-checking-debugging-and-profiling","title":"Testing, Checking, Debugging and Profiling","text":""},{"location":"language_guides/r/#testing-and-checking","title":"Testing and checking","text":"<p>Testthat is a testing package by Hadley Wickham. Testing chapter of a book R packages describes in detail testing process in R with use of <code>testthat</code>. Further, testthat: Get Started with Testing by Whickham may also provide a good starting point.</p> <p>See also checking and testing R packages. note that within RStudio R package check and R package test can be done via simple toolbar clicks.</p>"},{"location":"language_guides/r/#continuous-integration","title":"Continuous integration","text":"<p>Continuous integration should be done with an online service.</p>"},{"location":"language_guides/r/#debugging-and-profiling","title":"Debugging and Profiling","text":"<p>Debugging is possible in RStudio, see link. For profiling tips see link</p>"},{"location":"language_guides/r/#not-in-this-tutorial-yet","title":"Not in this tutorial yet:","text":"<ul> <li>Logging</li> </ul>"},{"location":"technology/datasets/","title":"Working with tabular data","text":"<p>Page maintainers: Suvayu Ali @suvayu , Flavio Hafner @f-hafner and Reggie Cushing @recap</p> <p>There are several solutions available to you as an RSE, with their own pros and cons.  You should evaluate which one works best for your project, and project partners, and pick one.  Sometimes it might be, that you need to combine two different types of technologies.  Here are some examples from our experience.</p> <p>You will encounter datasets in various file formats like: - CSV/Excel - Parquet - HDF5/NetCDF - JSON/JSON-LD</p> <p>Or local database files like SQLite.  It is important to note, the various trade-offs between these formats.  For instance, doing a random seek is difficult with a large dataset for non-binary formats like: CSV, Excel, or JSON.  In such cases you should consider formats like Parquet, or HDF5/NetCDF.  Non-binary files can also be imported into local databases like SQLite or DuckDB.  Below we compare some options to work with datasets in these formats.</p> <p>It's also good to know about Apache Arrow, which is not itself a file format, but a specification for a memory layout of (binary) data. There is an ecosystem of libraries for all major languages to handle data in this format. It is used as the back-end of many data handling projects, among which a few others mentioned in this chapter.</p>"},{"location":"technology/datasets/#local-database","title":"Local database","text":"<p>When you have a relational dataset, it is recommended that you use a database.  Using local databases like SQLite and DuckDB can be very easy because of no setup requirements. But they come with some some limitations; for instance, multiple users cannot write to the database simultaneously.</p> <p>SQLite is a transactional database, so if you have a dataset that is changing with time (e.g. you are adding new rows), it would be more appropriate.  However in research often we work with static databases, and are interested mostly in analytical tasks.  For such a case, DuckDB is a more appropriate alternative.  Between the two, - DuckDB can also create views (virtual tables) from other sources like files, other databases, but with SQLite you always have to import the data before running any queries. - DuckDB is multi-threaded. This can be an advantage for large databases, where aggregation queries tend to be faster than sqlite.    - However if you have a really large dataset, say 100Ms of rows, and want to perform a deeply nested query, it would require substantial amount of memory, making it unfeasible to run on personal laptops.    - There are options to customize memory handling, and push what is possible on a single machine.</p> <pre><code> You need to limit the memory usage to prevent the operatings system, or shell from preemptively killing it.  You can choose a value about 50% of your system's RAM.\n ```sql\n SET memory_limit = '5GB';\n ```\n By default, DuckDB spills over to disk when memory usage grows beyond the above limit.  You can verify the temporary directory by running:\n ```sql\n SELECT current_setting('temp_directory') AS temp_directory;\n ```\n Note, if your query is deeply nested, you should have sufficient disk space for DuckDB to use; e.g. for 4 nested levels of `INNER JOIN` combined with a `GROUP BY`, we observed a disk spill over of 30x the original dataset.  However we found this was not always reliable.\n\n In this kind of borderline cases, it might be possible to address the limitation by splitting the workload into chunks, and aggregating later, or by considering one of the alternatives mentioned below.\n</code></pre> <ul> <li>You can also optimize the queries for DuckDB, but that requires a deeper dive into the documentation, and understanding how DuckDB query optimisation works.</li> <li>Both databases support setting (unique) indexes. Indexes are useful and sometimes necessary</li> <li>For both DuckDB and SQLite, unique indexes allow to ensure data integrity</li> <li>For SQLite, indexes are crucial to improve the performance of queries. However, having more indexes makes writing new records to the database slower. So it's again a trade-off between query and write speed.</li> </ul>"},{"location":"technology/datasets/#useful-libraries","title":"Useful libraries","text":""},{"location":"technology/datasets/#database-apis","title":"Database APIs","text":"<ul> <li>SQLAlchemy</li> <li>In Python, interfacing to SQL databases like SQLite, MySQL or PostgreSQL is often done using SQLAlchemy, which is an Object Relational Mapper (ORM) that allows you to map tables to Python classes. Note that you still need to use a lot of manual SQL outside of Python to manage the database. However, SQLAlchemy allows you to use the data in a Pythonic way once you have the database layout figured out.</li> </ul>"},{"location":"technology/datasets/#data-processing-libraries-on-a-single-machine","title":"Data processing libraries on a single machine","text":"<ul> <li>Pandas</li> <li>The standard tool for working with dataframes, and widely used in analytics or machine learning workflows.  Note however how Pandas uses memory, because certain APIs create copies, while others do not.  So if you are chaining multiple operations, it is preferable to use APIs that avoid copies.</li> <li>Vaex</li> <li>Vaex is an alternative that focuses on out-of-core processing (larger than memory), and has some lazy evaluation capabilities.</li> <li>Polars</li> <li>An alternative to Pandas (started in 2020), which is primarily written in Rust.  Compared to pandas, it is multi-threaded and does lazy evaluation with query optimisation, so much more performant.  However since it is newer, documentation is not as complete.  It also allows you to write your own custom extensions in Rust.</li> <li>Apache Datafusion</li> <li>A very fast, extensible query engine for building high-quality data-centric systems in Rust, using the Apache Arrow in-memory format. DataFusion offers SQL and Dataframe APIs, excellent performance, built-in support for CSV, Parquet, JSON, and Avro, extensive customization, and a great community.</li> </ul>"},{"location":"technology/datasets/#distributedmulti-node-data-processing-libraries","title":"Distributed/multi-node data processing libraries","text":"<ul> <li>Dask</li> <li><code>dask.dataframe</code> and <code>dask.array</code> provides the same API as pandas and numpy respectively, making it easy to switch.</li> <li>When working with multiple nodes, it requires communication across nodes (which is network bound).</li> <li>Ray</li> <li>Apache Spark</li> </ul>"},{"location":"technology/gpu/","title":"GPU Programming Languages","text":"<p>Page maintainer: Alessio Sclocco @isazi</p>"},{"location":"technology/gpu/#learning-resources","title":"Learning Resources","text":"<ul> <li>Carpentries GPU Programming course</li> <li>Lesson material</li> <li>Introduction to CUDA C</li> <li>Slides</li> <li>Video</li> <li>Introduction to OpenACC</li> <li>Slides</li> <li>Introduction to HIP Programming</li> <li>Video</li> <li>SYCL Introduction and Best Practices</li> <li>Video</li> <li>CSCS GPU Programming with Julia</li> <li>Course recordings</li> </ul>"},{"location":"technology/gpu/#documentation","title":"Documentation","text":"<ul> <li>CUDA</li> <li>C programming guide</li> <li>Runtime API</li> <li>Driver API</li> <li>Fortran programming guide</li> <li>HIP</li> <li>Kernel language syntax</li> <li>Runtime API</li> <li>SYCL</li> <li>Specification</li> <li>Reference guide</li> <li>OpenCL</li> <li>Guide</li> <li>API</li> <li>OpenCL C specification</li> <li>Reference guide</li> <li>OpenACC</li> <li>Programming guide</li> <li>Reference guide</li> <li>OpenMP</li> <li>Reference guide</li> </ul>"},{"location":"technology/gpu/#overview-of-libraries","title":"Overview of Libraries","text":"<ul> <li>CUDA</li> <li>cuBLAS</li> <li>NVBLAS</li> <li>cuFFT</li> <li>cuGRAPH</li> <li>cuRAND</li> <li>cuSPARSE</li> <li>HIP</li> <li>hipBLAS</li> <li>hipFFT</li> <li>hipRAND</li> <li>hipSPARSE</li> <li>SYCL</li> <li>OneAPI BLAS</li> <li>OneAPI FFT</li> <li>OneAPI sparse</li> <li>OneAPI random number generators</li> <li>OpenCL</li> <li>CLBlast</li> <li>clFFT</li> </ul>"},{"location":"technology/gpu/#source-to-source-translation","title":"Source-to-source Translation","text":"<ul> <li>CUDA to HIP</li> <li>hipify</li> <li>CUDA to SYCL</li> <li>SYCLomatic</li> <li>CUDA to OpenCL</li> <li>cutocl</li> </ul>"},{"location":"technology/gpu/#foreign-function-interfaces","title":"Foreign Function Interfaces","text":"<ul> <li>C++</li> <li>CUDA<ul> <li>cudawrappers</li> </ul> </li> <li>OpenCL<ul> <li>CLHPP</li> </ul> </li> <li>Python</li> <li>CUDA<ul> <li>PyCuda</li> <li>CuPy</li> <li>cuda-python</li> </ul> </li> <li>HIP<ul> <li>PyHIP</li> </ul> </li> <li>SYCL<ul> <li>dpctl</li> </ul> </li> <li>OpenCL<ul> <li>PyOpenCL</li> </ul> </li> <li>Julia</li> <li>CUDA<ul> <li>CUDA.jl</li> </ul> </li> <li>HIP<ul> <li>AMDGPU.jl</li> </ul> </li> <li>SYCL<ul> <li>oneAPI.jl</li> </ul> </li> <li>Java</li> <li>CUDA<ul> <li>JCuda</li> </ul> </li> <li>OpenCL<ul> <li>JOCL</li> </ul> </li> </ul>"},{"location":"technology/gpu/#high-level-abstractions","title":"High-Level Abstractions","text":"<ul> <li>C++</li> <li>Kokkos</li> <li>Raja</li> <li>Python</li> <li>Numba</li> <li>pykokkos</li> </ul>"},{"location":"technology/gpu/#debugging-and-profiling-tools","title":"Debugging and Profiling Tools","text":"<ul> <li>CUDA</li> <li>Nsight Systems</li> <li>Nsight Compute</li> <li>CUDA-GDB</li> <li>compute-sanitizer</li> <li>HIP</li> <li>omniperf</li> <li>rocprof</li> <li>SYCL</li> <li>oneprof</li> <li>onetrace</li> </ul>"},{"location":"technology/gpu/#performance-optimization","title":"Performance Optimization","text":"<ul> <li>PRACE best practice guide on modern accelerators</li> <li>CUDA best practices</li> <li>OneAPI SYCL best practices</li> </ul>"},{"location":"technology/gpu/#auto-tuning","title":"Auto-tuning","text":"<ul> <li>Kernel Tuner</li> <li>GitHub repository</li> <li>Documentation</li> <li>Tutorial</li> </ul>"},{"location":"technology/technology_overview/","title":"Technology overview","text":"<p>Page maintainer: Patrick Bos @egpbos</p> <p>These chapters are based on our experiences with using specific software technologies.</p> <p>The main audience is RSEs familiar with basic computing and programming concepts.</p> <p>The purpose of these chapters is for someone unfamiliar with the specific technology to get a quick overview of the most important concepts, practices and tools, without going into too much detail (we provide links to further reading material for more).</p>"},{"location":"technology/user_experience/","title":"User Experience (UX)","text":"<p>Page maintainer: Jesus Garcia @ctwhome</p> <p>User Experience Design (UX) is a broad, holistic science that combines many cognitive and brain sciences disciplines like psychology and sociology, content strategies, and arts and aesthetics by following human-center approaches.</p> <p>Human-centred design is an approach to interactive systems development that aims to make systems usable and useful by focusing on the users, their needs and requirements, and applying human factors/ergonomics and usability knowledge and techniques. This approach enhances effectiveness and efficiency, improves human well-being, user satisfaction, accessibility, sustainability, and counteracts possible adverse effects on human health, safety, and performance. HCDSociety</p>"},{"location":"technology/user_experience/#table-of-content","title":"Table of content","text":"<ul> <li>UX disciplines</li> <li>Design thinking process</li> <li>Designing software</li> <li>Tools and Resources</li> </ul>"},{"location":"technology/user_experience/#ux-disciplines","title":"UX disciplines","text":"<p>The principles and indications taught by interaction-design.org can be useful in the process of creating research software.</p> <p>The main UX disciplines are:</p> <ol> <li>User research: understanding the people who use a product or system through observations.</li> <li>Information architecture: identifying and organizing information within a system in a purposeful and meaningful way.</li> <li>Interaction design: designing a product or system's interactive behaviors with a specific focus on their use.</li> <li>Usability evaluation: measuring the quality of a user's experience when interacting with a product or system.</li> <li>Accessibility evaluation: measuring the quality of a product or system to be accessed irrespective of personal abilities and device properties.</li> <li>Visual design: designing the visual attributes of a product or system in an aesthetically pleasing way.</li> </ol> <p>The known UX umbrella diagram represents the different disciplines of UX:</p> <p></p> <p>Author/Copyright holder: J.G. Gonzalez and The Netherlands eScience Center. Copyright: Apache License 2.0</p>"},{"location":"technology/user_experience/#design-thinking","title":"Design Thinking","text":"<p>Design thinking is an approach, mindset, or ideology for product development. According to the IxF(Interaction Design Foundation, Design thinking achieves all these advantages at the same time:</p> <ul> <li>It is a user-centered process that starts with user data, creates design artifacts that address real and not imaginary user needs, and then tests those artifacts with real users.</li> <li>It leverages the collective expertise and establishes a shared language and buy-in amongst your team.</li> <li>It encourages innovation by exploring multiple avenues for the same problem.</li> </ul> <p></p> <p>Author/Copyright holder: Teo Yu Siang and Interaction Design Foundation. Copyright licence: CC BY-NC-SA 3.0</p> <p>You can find more information about Design Thinking on the IxF page.</p>"},{"location":"technology/user_experience/#designing-software","title":"Designing software","text":"<p>Heuristics, or commonly known 'as the rule of thumb,' play a significant role when users interact with software. The Nielsen/Norman group has a top 10 Usability Heuristics for User Interface Design to consider when developing software.</p>"},{"location":"technology/user_experience/#designing-lovable-software","title":"Designing Lovable software","text":"<p>When delivering software iteratively, one of the common approaches to follow is to define a Minimum Value Product that contains the minimum requirements. Often is forgotten in this approach to deliver software that attracts and engages the users. When developing research software, researchers should present the new and innovative outcomes in a way that feels comfortable and easy to use from the very beginning, eliminating any cognitive burden that the software's interaction may include.</p> <p></p> <p>Author/Copyright holder: J.G. Gonzalez and The Netherlands eScience Center. Copyright: Apache License 2.0</p> <p>While MVP (Minumun Product Value) focuses on provide users with a way to explore the product and understand its main intent, MLP (Minimun Loveable Product) approach focuses on essential features instead of the bare minimum expected from a class software. Going beyond the bare functionality, the attention is driven towards a great user experience. The outcomes mush contains all elements in the pyramid being functional, reliable, usable, and pleasurable.</p>"},{"location":"technology/user_experience/#tools-and-resources","title":"Tools and resources","text":"<p>Design tools used for Visual Design, Prototyping, and IxD testing collaborative, real-time, online, and multiplatform.</p> <ul> <li>Figma</li> <li>Miro</li> <li>Whimsical</li> </ul>"}]}